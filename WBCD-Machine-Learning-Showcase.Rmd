---
title: \LARGE __WBCD Machine Learning Showcase__
author: \Large Carlos Outerelo
output:
    #pdf_document:
    #  citation_package: natbib
    #  df_print: kable
    #  fig_crop: no
    #  latex_engine: xelatex
    #  toc_depth: 4
    html_document:
      toc: yes
      toc_depth: 3
      toc_float: TRUE
      theme: united
      highlight: tango
      code_folding: show
      fig_width: 14
      fig_height: 10
header-includes:
    - \usepackage {hyperref}
    - \hypersetup {colorlinks = true, linkcolor = blue, urlcolor = blue}
---

```{r setup, include=FALSE}
if(!require(htmltools)) {
  install.packages("htmltools")
}
library(htmltools)

if(!require(knitr)) {
  install.packages("knitr")
}
library(knitr)

knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      collapse = TRUE
                      #tidy.opts = list(width.cutoff=80), tidy=TRUE
                      )
```

```{css, echo=FALSE}
br {
	line-height: 30px;
}

li {
	margin-top: 10px;
	margin-bottom: 10px;
}

ul, ol {
	margin-bottom: 15px;
}

.r-code-collapse {
	margin-bottom: 25px;
}

h1, h2, h3, h4 {
	margin-top: 25px;
	margin-bottom: 25px;
}

h4 {
	margin-bottom: 20px;
	font-size: 20px;
	font-weight: 500;
}

.author, .quoted {
  margin-left: 20px;
  font-style: italic; 
  color: rgb(92, 92, 92);
}

.boolean {
  color: rgb(155, 0, 192);
}

.string {
  color: rgb(0, 160, 0);
}

.quote {
  text-align: center;
  font-size: 16px;
  font-weight: 700;
  color: rgb(72, 61, 139);
}

.tocify-extend-page {
  display: none
}
```

\newpage
\tableofcontents

<br>

\newpage
# Preface
This project aims to be as educational and detailed as possible to showcase my understanding and proficiency of [the R programming language](https://en.wikipedia.org/wiki/R_(programming_language)), [R Markdown](https://rmarkdown.rstudio.com/) and the topics at hand: [data science](https://en.wikipedia.org/wiki/Data_science) (exploration, visualization, PCA...) and [machine learning](https://en.wikipedia.org/wiki/Machine_learning) (through several high level libraries).  
Note that the R Markdown file outputs a .HTML document that has been [uplodad to RPubs](https://rpubs.com/outerelocarlos/WBCD-Machine-Learning-Showcase) where it can be properly visualized. Both files are also available in their dedicated [GitHub repository](https://rpubs.com/outerelocarlos/WBCD-Machine-Learning-Showcase).<br>

The goal of this project is to perform a complete exploratory data analysis upon the Wisconsin Breast Cancer Dataset describing and detailing each step of the process. The document and its content are heavily based upon the work of [Miri Choi in kaggle](https://www.kaggle.com/mirichoi0218/classification-breast-cancer-or-not-with-15-ml/report "[Classification] Breast Cancer or Not") although with a much larger scope, much more detail (throughfully commenting each step of the process) and further developed (using additional machine learning libraries and approaches).

\newpage
# 1. Setup

## 1.1. Project libraries
As previously stated, this project uses [the R programming language](https://en.wikipedia.org/wiki/R_(programming_language)) along with several libraries which, as is the norm in most non-basic R projects, are often required to complement R with additional (and specific) functions. The following code snippet installs all of the required libraries if they are not installed already (through the use of conditionals and the built-in [require()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/library) function), which are presented in alphabetical order for the sake of convenience (note that the installation itself is called by the [install.packages()](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/install.packages) function).

```{r libraries_install, eval = FALSE}
if(!require(ade4)) {
  install.packages("ade4")
}
if(!require(caret)) {
  install.packages("caret")
}
if(!require(C50)) {
  install.packages("C50")
}
if(!require(corrplot)) {
  install.packages("corrplot")
}
if(!require(data.table)) {
  install.packages("data.table")
}
if(!require(dplyr)) {
  install.packages("dplyr")
}
if(!require(ExPosition)) {
  install.packages("ExPosition")
}
if(!require(factoextra)) {
  install.packages("factoextra")
}
if(!require(FactoMineR)) {
  install.packages("FactoMineR")
}
if(!require(GGally)) {
  install.packages("GGally")
}
if(!require(ggplot2)) {
  install.packages("ggplot2")
}
if(!require(gridExtra)) {
  install.packages("gridExtra")
}
if(!require(highcharter)) {
  install.packages("highcharter")
}
if(!require(PerformanceAnalytics)) {
  install.packages("PerformanceAnalytics")
}
if(!require(PST)) {
  install.packages("PST")
}
if(!require(psych)) {
  install.packages("psych")
}
if(!require(RCurl)) {
  install.packages("RCurl")
}
```

Installing a given package does not mean said package (and its associated functions) are yet ready to be used. To do so, it needs to be properly loaded into the R workspace, for which there exists the built-in [library()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/library) function. The following code snippet makes use of said function to import/load all of the project's required libraries (once again, in alphabetical order for the sake of convenience).

```{r libraries_load}
library(ade4)
library(caret)
library(C50)
library(corrplot)
library(data.table)
library(dplyr)
library(ExPosition)
library(factoextra)
library(FactoMineR)
library(GGally)
library(ggplot2)
library(gridExtra)
library(highcharter)
library(PerformanceAnalytics)
library(PST)
library(psych)
library(RCurl)
```

Note that some of these libraries are also included in the [<strong>tidyverse</strong>](https://www.rdocumentation.org/packages/tidyverse/) package. However, I rather understand the use-case scenario of each instead of relying on library bundles.

## 1.2. Importing the dataset
The next step after installing and loading all of the required packages is to load the data itself. To do so, an optional variable (refered to as <code>urlfile</code> in the upcoming code snippet) can be created so that it holds the the URL (string) from which to mine the data - doing so requires the use of the <code>read.csv()</code> function (or an equivalent function from an alternative library).<br>

It is highly recommended to explore the .data file before importing its content into the work environment in order to check whether or not the data is preceded by a header. In this case it is not, so using the argument <code>header = <span class="boolean">FALSE</span></code> ensures the data is read properly.<br>

More information about the <code>read.csv()</code> function, its behavior and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/read.table <br>

The function <code>head()</code> prints the very first rows of the loaded dataset, which is useful to observe how the data has been interpreted by R.

```{r import_data_1}
urlfile = "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
wbcd <- read.csv(urlfile, header = FALSE)
head(wbcd)
```

The function <code>colnames()</code> combines a string vector of length "N" with a dataset of "N" columns so that the elements inside the vector become the dataset's header. Since the lack of content-descriptive column headers in the Wisconsin Breast Cancer Dataset makes the data hard to understand, the following code snippet makes use of <code>colnames()</code> to properly name each of the columns.<br>

More information about the <code>colnames()</code> function, its behavior and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/row%2Bcolnames

```{r colnames}
colnames(wbcd) <- c("id","diagnosis","radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean","compactness_mean","concavity_mean",
                    "concave points_mean","symmetry_mean","fractal_dimension_mean","radius_se","texture_se","perimeter_se","area_se","smoothness_se",
                    "compactness_se","concavity_se","concave points_se","symmetry_se","fractal_dimension_se","radius_worst","texture_worst","perimeter_worst",
                    "area_worst","smoothness_worst","compactness_worst","concavity_worst","concave points_worst","symmetry_worst","fractal_dimension_worst")
head(wbcd)
```

After checking that the data is now properly referenced by the new header, it is time to perform a proper examination of the dataset through the use of the <code>str()</code> and the <code>summary()</code> functions. The function <code>class()</code> is redundant in this case, as the function <code>str()</code> already returns the class of the dataset alongside the class and first elements of the many columns/variables which define the dataset (in other words, <code>str()</code> displays, in a compact manner, the internal structure of an R object) - however, it is included in the upcoming code snippet due to its usefulness within other scenarios (and given that, as stated previously, this extensive analysis aims to be educational and informative). <br>

The <code>summary()</code> function, on the other hand, produces result summaries of the results of various model fitting functions: it returns the minimum, maximum, mean, median along with the first and third quartiles of any numeric-based columns/variables (for factor-based columns/variables such as the <code>diagnosis</code> one, it returns the occurrence of each of the factors).

More information about these functions can be found in their associated RDocumentation page:
<ul>
  <li><code>class()</code>: https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/class </li>
  <li><code>str()</code>: https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/str </li>
  <li><code>summary()</code>: https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary </li>
</ul>

```{r examining_data_1}
class(wbcd)
str(wbcd)
summary(wbcd)
```

The code snippet available right above showcases the dataset' class, structure and overall summary - as expected from the functions at play. For the purposes of this document, the observations worth highlighting at this point are the following:
<ul>
  <li>There is an <code>id</code> column which holds no valuable information.</li>
  <li>Every tumor measurement appears thrice (e.g., there is a <code>perimeter_mean</code>, a <code>perimeter_se</code> and a <code>perimeter_worst</code> - each with its own mean, standard error and maximum and minimum values).</li>
</ul>

## 1.3. Data wrangling
Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one "raw" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.<br>

Since the dataset at hand is already of class <code>data.frame</code>, there is no need to transform its class in any way (the functions to be used work with <code>data.frame</code> class objects). Given that, the next procedure is to clean any empty values (<code class = "boolean">NULL</code> and <code class = "boolean">NAs</code> through the use of the <code>is.null()</code> and <code>is.na()</code> functions) which could meddle with later operations - let that be the first wrangling operation to be performed:

```{r cleaning_empty_1}
# Check for NULL data
null_check <- c()
for (i in 1:dim(wbcd)[1]) {
  for (j in 1:dim(wbcd)[2]) {
    # print(wbcd[i,j]) # This print() snippet allows to check each individual value passed onto this loop
    append(null_check, is.null(wbcd[i,j]))
  }
}
null_check # No NULL data (null_check = NULL)

# Check for NA data
na_check <- c()
for (i in 1:dim(wbcd)[1]) {
  for (j in 1:dim(wbcd)[2]) {
    # print(wbcd[i,j]) # This print() snippet allows to check each individual value passed onto this loop
    append(null_check, is.na(wbcd[i,j]))
  }
}
na_check # No NA data (na_check = NULL)
```

As stated previously, there is no need for the <code>id</code> column within our dataset (since it holds no valuable information), so the next (and final) wrangling procedure is to get rid of it. The following code snippet showcases an addition optional step, albeit a recommended one: the factors' nomenclature are changed to better convey its meaning (this helps those unfamiliarized with the dataset to better interpret it).

```{r wrangling_1}
wbcd <- wbcd[,-1]
wbcd$diagnosis <- factor(ifelse(wbcd$diagnosis=="B","Benign","Malignant"))
```

# 2. Data Exploration

## 2.1. Correlation Charts
Data analysts/scientists aim to study and understand a given set of data - correlation charts facilitate said study, clearly showing which variables are independent and which are not. What's more, these correlations are core for the Principal Component Analysis (PCA) which is to be performed (and detailed) later on this document.<br>

As stated previously, examining the data allows the user to appreciate that every tumor measurement appears thrice (e.g., there is a <code>perimeter_mean</code>, a <code>perimeter_se</code> and a <code>perimeter_worst</code> variables - each with its own mean, standard error and maximum and minimum values). Given that, this correlation chart exercise is performed upon 3 distinct groups:
<ul>
  <li>One with mean variables.</li>
  <li>One with standard error (se) variables.</li>
  <li>One with worst variables.</li>
</ul>

There are also multiple functions which can be used to plot the correlation charts - this document covers the following:
<ul>
  <li>The <code>chart.Correlation()</code> function from the <strong>PerformanceAnalytics</strong> package.</li>
  <li>The <code>pairs.panels()</code> function from the <strong>psych</strong> package.</li>
  <li>The <code>ggpairs()</code> function from the <strong>GGally</strong> package.</li>
  <li>The <code>ggcorr()</code> function from the <strong>GGally</strong> package.</li>
</ul>

### 2.1.1. chart.Correlation()
The <code>chart.Correlation()</code> function from the <strong>PerformanceAnalytics</strong> package allows the user to plot a correlation chart based on the arguments at play, which are the following:
<ul>
  <li><strong>R</strong>: data to correlate against itself (it can either be a vector, a matrix or a timeseries).</li>
  <li><strong>histogram</strong>: <code class="boolean">TRUE</code> or <code class="boolean">FALSE</code> whether or not to display a histogram.</li>
  <li><strong>method</strong>: a character string indicating which correlation coefficient (or covariance) is to be computed. Options are <code class = "string">"pearson"</code> (default), <code class = "string">"kendall"</code> and <code class = "string">"spearman"</code>.</li>
</ul>

```{r chart_Correlation_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(PerformanceAnalytics)
chart.Correlation(R, 
                  histogram = TRUE, 
                  method = "pearson", 
                  ...)
```

Additional arguments can be passed through in order to better define the aesthetic elements of the scatter plots and optional histogram - the function accepts any arguments that can passed through into <code>pairs</code>. Further information regarding the function <code>chart.Correlation()</code>, its behavior and its arguments is available in its RDocumentation associated page: https://www.rdocumentation.org/packages/PerformanceAnalytics/versions/2.0.4/topics/chart.Correlation <br>

The following code snippet showcases the function at hand, although only Pearson correlation is plotted for the sake of simplicity and readability.

```{r chart_Correlation_1, eval=FALSE}
library(PerformanceAnalytics)

# Analysis of the Pearson correlation between variables
chart.Correlation(wbcd[,c(2:11)], 
                  histogram=TRUE, 
                  method = "pearson",
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for mean data")

chart.Correlation(wbcd[,c(12:21)], 
                  histogram=TRUE, 
                  method = "pearson",
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for se data")

chart.Correlation(wbcd[,c(22:31)], 
                  histogram=TRUE,
                  method = "pearson",
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for worst data")

# Analysis of the Kendall correlation between variables
chart.Correlation(wbcd[,c(2:11)], 
                  method = "kendall", 
                  histogram=TRUE, 
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for mean data")

chart.Correlation(wbcd[,c(12:21)], 
                  method = "kendall", 
                  histogram=TRUE, 
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for se data")

chart.Correlation(wbcd[,c(22:31)], 
                  method = "kendall", 
                  histogram=TRUE, 
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for worst data")

# Analysis of the Spearman correlation between variables
chart.Correlation(wbcd[,c(2:11)],
                  method = "spearman", 
                  histogram=TRUE, 
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for mean data")

chart.Correlation(wbcd[,c(12:21)], 
                  method = "spearman", 
                  histogram=TRUE, 
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for se data")

chart.Correlation(wbcd[,c(22:31)],
                  method = "spearman", 
                  histogram=TRUE, 
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for worst data")
```

#### {.tabset}

##### Mean
```{r chart_Correlation_1_1, echo=FALSE}
chart.Correlation(wbcd[,c(2:11)], 
                  histogram=TRUE,
                  method = "pearson",
                  col="grey10",
                  pch=1,
                  main="Correlation chart for mean data")
```

##### SE
```{r chart_Correlation_1_2, echo=FALSE}
chart.Correlation(wbcd[,c(12:21)], 
                  histogram=TRUE,
                  method = "pearson",
                  col="grey10", 
                  pch=1,
                  main="Correlation chart for se data")
```

##### Worst
```{r chart_Correlation_1_3, echo=FALSE}
chart.Correlation(wbcd[,c(22:31)], 
                  histogram=TRUE,
                  method = "pearson",
                  col="grey10",
                  pch=1,
                  main="Correlation chart for worst data")
```

#### {-}

### 2.1.2. pairs.panels()
The <code>pairs.panels()</code> function from the <strong>psych</strong> package allows the user to plot a correlation chart based on the arguments at play, which are the following:
<ul>
  <li><strong>x</strong>: data to correlate against itself (a matrix or <code>data.frame</code>).</li>
  <li><strong>smooth</strong>: <code class="boolean">TRUE</code> draws loess smooths.</li>
  <li><strong>scale</strong>: <code class="boolean">TRUE</code> scales the correlation font by the size of the absolute correlation.</li>
  <li><strong>density</strong>: <code class="boolean">TRUE</code> shows the density plots as well as histograms.</li>
  <li><strong>ellipses</strong>: <code class="boolean">TRUE</code> draws correlation ellipses.</li>
  <li><strong>lm</strong>: <code class="boolean">TRUE</code> plots the linear fit rather than the LOESS smoothed fits.</li>
  <li><strong>digits</strong>: the number of digits to show.</li>
  <li><strong>method</strong>: a character string indicating which correlation coefficient (or covariance) is to be computed. Options are <code class = "string">"pearson"</code> (default), <code class = "string">"kendall"</code> and <code class = "string">"spearman"</code>.</li>
  <li><strong>pch</strong>: the plot character (defaults to 20 which is a '.').</li>
  <li><strong>cor</strong>: <code class="boolean">TRUE</code> or <code class="boolean">FALSE</code> determines whether or not to report correlations if plotting regressions.</li>
  <li><strong>jiggle</strong>: <code class="boolean">TRUE</code> or <code class="boolean">FALSE</code> determines whether or not the data points are jittered before being plotted.</li>
  <li><strong>factor</strong>: factor for jittering (1-5)</li>
  <li><strong>hist.col</strong>: defines the histogram's color.</li>
  <li><strong>show.points</strong>: if <code class="boolean">FALSE</code>, do not show the data points, just the data ellipses and smoothed functions.</li>
  <li><strong>rug</strong>: <code class="boolean">TRUE</code> or <code class="boolean">FALSE</code> determines whether or not a rug is drawed under the histograms.</li>
  <li><strong>breaks</strong>: if specified, allows control for the number of breaks in the histogram.</li>
  <li><strong>cex.cor</strong>: if one just specifies <code>cex</code> then the argument will only determines the size of the text in the correlation's boxes, but if <code>cex.cor</code> is specified then the argument will function to change the points' size.</li>
  <li><strong>wt</strong>: if specified, then weight the correlations by a weights matrix.</li>
  <li><strong>smoother</strong>: if <code class="boolean">TRUE</code>, then <code>smooth.scatter</code> is applied upon the data points (which is slow but pretty when lots of subjects are to be plotted).</li>
  <li><strong>stars</strong>: <code class="boolean">TRUE</code> or <code class="boolean">FALSE</code> determines whether or not to show the significance of correlations by using astricks [*].</li>
  <li><strong>ci</strong>: <code class="boolean">TRUE</code> or <code class="boolean">FALSE</code> determines whether or not to draw confidence intervals for the linear model or for the loess fit. If confidence intervals are not drawn, the fitting function is lowess.</li>
  <li><strong>alpha</strong>: the alpha level for the confidence regions.</li>
</ul>

```{r pairs_panels_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(psych)
pairs.panels(x, 
             smooth = TRUE, 
             scale = FALSE, 
             density = TRUE, 
             ellipses = TRUE, 
             lm = FALSE, 
             digits = 2, 
             method = "pearson",
             pch = 20,
             cor = TRUE,
             jiggle = FALSE, 
             factor = 2, 
             hist.col = "cyan", 
             show.points = TRUE, 
             rug = TRUE,
             breaks = "Sturges",
             cex.cor = 1,
             wt = NULL,
             smoother = FALSE, 
             stars = FALSE, 
             ci = FALSE, 
             alpha = .05, 
             ...)
```

Like with the <code>chart.Correlation()</code> function, additional arguments can be passed through in order to better define the scatter plot and the histogram, which is not optional with this function. Besides that, the function <code>pairs.panels()</code> can be considered more customizable than <code>chart.Correlation()</code> due to the sheer amount of specific arguments that can be used with it. Note that, as was the case with <code>chart.Correlation()</code>, the function <code>pairs.panels()</code> also accepts any arguments that can passed through into <code>pairs</code>.<br>

For more information about the function itself, here's the rdocumentation.org related page: https://www.rdocumentation.org/packages/psych/versions/2.1.6/topics/pairs.panels <br>

The following code snippet showcases the function at hand, although only Pearson correlation is plotted for the sake of simplicity and readability.
  
```{r pairs_panels_1, eval=FALSE}
library(psych)

# Analysis of the Pearson correlation between variables
pairs.panels(wbcd[,c(2:11)], 
             method="pearson", 
             hist.col = "#cccccc", 
             pch=1, lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for mean data")

pairs.panels(wbcd[,c(12:21)],
             method="pearson",
             hist.col = "#cccccc", 
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for se data")

pairs.panels(wbcd[,c(22:31)], 
             method="pearson", 
             hist.col = "#cccccc", 
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for worst data")

# Analysis of the Kendall correlation between variables
pairs.panels(wbcd[,c(2:11)], 
             method="kendall",
             hist.col = "#cccccc",
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for mean data")

pairs.panels(wbcd[,c(12:21)], 
             method="kendall", 
             hist.col = "#cccccc", 
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for se data")

pairs.panels(wbcd[,c(22:31)], 
             method="kendall", 
             hist.col = "#cccccc", 
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for worst data")

# Analysis of the Spearman correlation between variables
pairs.panels(wbcd[,c(2:11)], 
             method="spearman", 
             hist.col = "#cccccc",
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for mean data")

pairs.panels(wbcd[,c(12:21)], 
             method="spearman", 
             hist.col = "#cccccc",
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for se data")

pairs.panels(wbcd[,c(22:31)], 
             method="spearman", 
             hist.col = "#cccccc", 
             pch=1, 
             lm=TRUE, 
             stars = TRUE,
             main="Correlation chart for worst data")
```

#### {.tabset}

##### Mean
```{r pairs_panels_1_1, echo=FALSE}
pairs.panels(wbcd[,c(2:11)], 
             method="pearson", 
             hist.col = "#cccccc", 
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for mean data")
```

##### SE
```{r pairs_panels_1_2, echo=FALSE}
pairs.panels(wbcd[,c(12:21)], 
             method="pearson", 
             hist.col = "#cccccc",
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for se data")
```

##### Worst
```{r pairs_panels_1_3, echo=FALSE}
pairs.panels(wbcd[,c(22:31)], 
             method="pearson", 
             hist.col = "#cccccc",
             pch=1, 
             lm=TRUE, 
             stars = TRUE, 
             main="Correlation chart for worst data")
```

#### {-}

### 2.1.3. ggpairs()
The <code>ggpairs()</code> function from the <strong>GGally</strong> package allows the user to plot a correlation chart based on the arguments at play, among which it is worth noting the following:
<ul>
  <li><strong>x</strong>: dataset to correlate against itself (can have both numerical and categorical data).</li>
  <li><strong>mapping</strong>: aesthetic mapping.</li>
  <li><strong>columns</strong>: which columns are used to make plots. Defaults to all columns.</li>
  <li><strong>title, xlab, ylab</strong>: title for the graph and labels for the x and y axis (respectively).</li>
</ul>

```{r ggpairs_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(GGally)
ggpairs(x, 
        mapping = NULL, 
        columns = 1:ncol(data),
        title = NULL,
        xlab = NULL,
        ylab = NULL,
        ...)
```

Like with the previous functions, additional arguments can be passed through. Fact is that <code>ggpairs()</code> is based upon <strong>ggplot2</strong>, which makes it compatible with many of the arguments available for said kind of plots and it would be besides the scope of this document to cover them all. Some of which are showcased in the upcoming code snippet, but more information about the function and its potential arguments can be found in its associated RDocumentation page: https://www.rdocumentation.org/packages/GGally/versions/1.5.0/topics/ggpairs <br>

Beware that <strong>ggplot2</strong> is one of the most powerful R tools to create any sort of graphics (arguably the most powerful one). The following RDocumentation page overviews its installation and usage: https://www.rdocumentation.org/packages/ggplot2/versions/3.3.5 <br>

The following code snippet showcases the function at hand. Note that, as oppossed to the previously detailed functions, <code>ggpairs()</code> does not allow the user to specify which correlation methodology to apply to the dataset - it applies Pearson's and that's about it.

#### {.tabset}

```{r ggpairs_1, eval = FALSE}
library(GGally)

ggpairs(wbcd[,c(2:11)],) + 
  theme_bw() +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))

ggpairs(wbcd[,c(12:21)],) + 
  theme_bw() +
  labs(title = "Correlation chart for se data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))

ggpairs(wbcd[,c(22:31)],) + 
  theme_bw() +
  labs(title = "Correlation chart for worst data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))
```

##### Mean
```{r ggpairs_1_1, echo = FALSE}
ggpairs(wbcd[,c(2:11)],) + 
  theme_bw() +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))
```

##### SE
```{r ggpairs_1_2, echo = FALSE}
ggpairs(wbcd[,c(12:21)],) + 
  theme_bw() +
  labs(title = "Correlation chart for se data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))
```

##### Worst
```{r ggpairs_1_3, echo = FALSE}
ggpairs(wbcd[,c(22:31)],) + 
  theme_bw() +
  labs(title = "Correlation chart for worst data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))
```

#### {-}

<br>

Being based upon <strong>ggplot2</strong> is a strong point in favor of <code>ggpairs()</code> and creativity can go a long way: data science is not only about understanding the data at hand but also to make it understandable for others - the many possibilities <strong>ggplot2</strong> brings along can dramatically increase the plot's readability thus making it easier to be understood by non-specialists (as in not data scientists). The upcoming code snippet slightly modifies the previous one showcasing a diagnosis-based coloring which makes the graph way easier to interpret (while also highlighting the striking similarities between this function' structure and <code>ggplot()</code>'s).

```{r ggpairs_color_1, eval = FALSE}
library(GGally)

ggpairs(wbcd[,c(2:11,1)], aes(color = diagnosis, alpha = 0.75), lower = list(continuous = "smooth")) + 
  theme_bw() +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))

ggpairs(wbcd[,c(12:21,1)], aes(color = diagnosis, alpha = 0.75), lower = list(continuous = "smooth")) + 
  theme_bw() +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))

ggpairs(wbcd[,c(22:31,1)], aes(color = diagnosis, alpha = 0.75), lower = list(continuous = "smooth")) + 
  theme_bw() +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))
```

#### {.tabset}

##### Mean
```{r ggpairs_color_1_1, echo = FALSE, fig.height = 12}
ggpairs(wbcd[,c(2:11,1)], aes(color = diagnosis, alpha = 0.75), lower = list(continuous = "smooth")) + 
  theme_bw() +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))
```

##### SE
```{r ggpairs_color_1_2, echo = FALSE, fig.height = 12}
ggpairs(wbcd[,c(12:21,1)], aes(color = diagnosis, alpha = 0.75), lower = list(continuous = "smooth")) + 
  theme_bw() +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))
```

##### Worst
```{r ggpairs_color_1_3, echo = FALSE, fig.height = 12}
ggpairs(wbcd[,c(22:31,1)], aes(color = diagnosis, alpha = 0.75), lower = list(continuous = "smooth")) + 
  theme_bw() +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = '#000000', hjust = 0.5, size = 14))
```

#### {-}

### 2.1.4. ggcorr() {.tabset}
The <code>ggcorr()</code> function from the <strong>GGally</strong> package allows the user to plot a simplified correlation chart focused solely on correlation values, which increases their visibility and readability (helping to illustrate and showcase certain points) at the cost of omitting the data itself (neither the datapoints and their scatter plot nor the histograms are plotted with this function).<br>

As with previous functions, the plot is based on the arguments at play which are the following:
<ul>
  <li><strong>data</strong>: a data frame or matrix containing numeric (continuous) data.</li>
  <li><strong>method</strong>: a vector of two character strings. The first value gives the method for computing covariances in the presence of missing values and must be one of <code class = "string">"everything"</code>, <code class = "string">"all.obs"</code>, <code class = "string">"complete.obs"</code>, <code class = "string">"na.or.complete"</code> or <code class = "string">"pairwise.complete.obs"</code> (abbreviations work); the second value gives the type of correlation coefficient to compute, and must be one of <code class = "string">"pearson"</code>, <code class = "string">"kendall"</code> or <code class = "string">"spearman"</code>.</li>
  <li><strong>cor_matrix</strong>: the named correlation matrix to use for calculations. Defaults to the correlation matrix of data when data is supplied.</li>
  <li><strong>nbreaks</strong>: the number of breaks to apply to the correlation coefficients, which results in a categorical color scale. Defaults to <code class = "boolean">NULL</code> which implies no breaks (continuous scaling).</li>
  <li><strong>digits</strong>: the number of digits to show in the breaks of the correlation coefficients.</li>
  <li><strong>low</strong>: the lower color of the gradient for continuous scaling of the correlation coefficients.</li>
  <li><strong>mid</strong>: the midpoint color of the gradient for continuous scaling of the correlation coefficients.</li>
  <li><strong>high</strong>: the upper color of the gradient for continuous scaling of the correlation coefficients.</li>
  <li><strong>midpoint</strong>: the midpoint value for continuous scaling of the correlation coefficients.</li>
  <li><strong>palette</strong>: if <code>nbreaks</code> is used, a ColorBrewer palette to use instead of the colors specified by <code>low</code>, <code>mid</code> and <code>high</code></li>
  <li><strong>geom</strong>: the geom object to use. Accepts either <code class = "string">"tile"</code>, <code class = "string">"circle"</code>, <code>"text"</code> or <code class = "string">"blank"</code>.</li>
  <li><strong>min_size</strong>: when <code>geom</code> has been set to <code class = "string">"circle"</code>, the minimum size of the circles.</li>
  <li><strong>max_size</strong>: when <code>geom</code> has been set to <code class = "string">"circle"</code>, the maximum size of the circles.</li>
  <li><strong>label</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether or not to add correlation coefficients to the plot.</li>
  <li><strong>label_alpha, label_color, label_round, label_size</strong></li>: aesthetic components for the label.</li>
  <li><strong>limits</strong>: bounding of color scaling for correlations, set <code>limits = <span class = "boolean">NULL</span></code> or <code class = "boolean">FALSE</code> to remove.</li>
  <li><strong>drop</strong>: if using nbreaks, <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether or not to drop unused breaks from the color scale.</li>
  <li><strong>layout.exp</strong>: a multiplier to expand the horizontal axis to the left if variable names get clipped.</li>
  <li><strong>legend.position</strong>: where to put the legend of the correlation coefficients.</li>
  <li><strong>legend.size</strong>: the size of the legend title and labels.</li>
</ul>

As was the case with <code>ggpairs()</code>, this function is also based upon ggplot2 making it compatible with many of ggplot2's arguments. More information regarding the function and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/GGally/versions/1.5.0/topics/ggcorr

```{r ggcorr_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(GGally)
ggcorr(data,
       method = c("pairwise", "pearson"),
       cor_matrix = NULL,
       nbreaks = NULL,
       digits = 2,
       name = "",
       low = "#3B9AB2",
       mid = "#EEEEEE",
       high = "#F21A00",
       midpoint = 0,
       palette = NULL,
       geom = "tile",
       min_size = 2,
       max_size = 6,
       label = FALSE,
       label_alpha = FALSE,
       label_color = "black",
       label_round = 1,
       label_size = 4,
       limits = c(-1, 1),
       drop = is.null(limits) || identical(limits, FALSE),
       layout.exp = 0,
       legend.position = "right",
       legend.size = 9,
       ...)
```

The following code snippets showcase the function at hand being applied to the Wisconsin Breast Cancer Dataset, although only Pearson correlation is plotted for the sake of simplicity and readability.

```{r ggcorr_1, eval = FALSE}
library(GGally)

# Analysis of the Pearson correlation between variables using the pairs.panels() function from the psych package
ggcorr(wbcd[,c(2:11)],
       method = c("pairwise", "pearson"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))

ggcorr(wbcd[,c(12:21)],
       method = c("pairwise", "pearson"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for se data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))

ggcorr(wbcd[,c(22:31)],
       method = c("pairwise", "pearson"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for worst data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))

# Analysis of the Pearson correlation between variables using the pairs.panels() function from the psych package
ggcorr(wbcd[,c(2:11)],
       method = c("pairwise", "kendall"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))

ggcorr(wbcd[,c(12:21)],
       method = c("pairwise", "kendall"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for se data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))

ggcorr(wbcd[,c(22:31)],
       method = c("pairwise", "kendall"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for worst data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))

# Analysis of the Pearson correlation between variables using the pairs.panels() function from the psych package
ggcorr(wbcd[,c(2:11)],
       method = c("pairwise", "spearman"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))

ggcorr(wbcd[,c(12:21)],
       method = c("pairwise", "spearman"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for se data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))

ggcorr(wbcd[,c(22:31)],
       method = c("pairwise", "spearman"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for worst data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))
```

#### Mean
```{r ggcorr_1_1, echo = FALSE}
ggcorr(wbcd[,c(2:11)],
       method = c("pairwise", "pearson"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for mean data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))
```

#### SE
```{r ggcorr_1_2, echo = FALSE}
ggcorr(wbcd[,c(12:21)],
       method = c("pairwise", "pearson"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for se data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))
```

#### Worst
```{r ggcorr_1_3, echo = FALSE}
ggcorr(wbcd[,c(22:31)],
       method = c("pairwise", "pearson"),
       name = "corr",
       geom = "tile",
       label = TRUE) +
  theme(legend.position = "none") +
  labs(title = "Correlation chart for worst data") +
  theme(plot.title = element_text(face = 'bold', color = 'black', hjust = 0.5, size = 12))
```

## 2.2. Principal Component Analysis
Principal Component Analysis, mostly known as PCA, is the most popular approach to dimensional reduction. It is widely used to summarize and to visualize the information within a dataset, often described through multiple inter-correlated quantitative variables. Considering each variable a dimension turns any dataset into a multi-dimensional matrix/hyperspace which is impossible to visualize when said dataset is defined by more than 3 variables (since there exist only 3 spatial dimensions) - that is where dimensional reduction (and thus, PCA) comes into play.<br>

Through PCA, the important information from a multivariate dataset can be extracted and expressed through a set of new variables called <strong>Principal Components</strong> (PC - singular; PCs - plural). These PCs are linear combinations of the original variables, so expressing the dataset using these PCs as variables reduces the overall number of variables needed to understand the data. Using this approach, PCA reduces the dimensionality of a multivariate dataset in order to identify patterns and better understand these highly complex matrices. This process also allows to visualize said multivariate datasets with minimal loss of information by reducing the number of PCs down to either 2 (for a bi-dimensional plot) or 3 (for a three-dimensional one).<br>

There's a fantastic video by content creator Josh Starmer which explains PCA step by step, detailing the intricacies of this technique and all of the elements at play. There are many aspects of PCA that will not be covered by the scope of this documents due to time constraints and to keep the overall size within reasonable limits, so the video essay in question is highly recommended as an excellent starting point: https://www.youtube.com/watch?v=FgakZw6K1QQ <br>

Several functions from different packages can be used to perform a PCA:
<ul>
  <li><code>prcomp()</code> and <code>princomp()</code>, which are R built-in functions</li>
  <li><code>PCA()</code> from the <strong>FactoMineR</strong> package</li>
  <li><code>dudi.pca()</code> from the <strong>ade4</strong> package</li>
  <li><code>epPCA()</code> from the <strong>ExPosition</strong> package</li>
</ul>

### 2.2.1. PCA with R built-in functions
As already stated, there are two built-in R functions (from within R's built-in <strong>stats</strong> package) that allow the user to perform a Principal Component Analysis: <code>prcomp()</code> and <code>princomp()</code>. Despite the similarities in name, their approaches to PCA are quite different: <code>prcomp()</code> is a singular value decomposition (SVD) which means that it is based upon the covariances/correlations between individuals, whereas <code>princomp()</code> is a spectral decomposition which instead examines the covariances/correlations between variables.<br>

It is worth noting that, according to the R help module, SVD has slightly better numerical accuracy and that makes <code>prcomp()</code> is the preferred approach (although there could exist a scenario under which <code>princomp()</code> yields better results).<br>

Let's take a look at the main arguments for both functions:

```{r R_pca, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
prcomp(x, 
       retx = TRUE, 
       center = TRUE, 
       scale. = FALSE,
       tol = NULL, 
       rank. = NULL, 
       ...)

princomp(x, 
         cor = FALSE, 
         scores = TRUE, ...)
```

Let's detail the arguments for <code>prcomp()</code>:
<ul>
  <li><strong>x</strong>: the numeric matrix or dataset/dataframe upon which to perform the PCA.</li>
  <li><strong>retx</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether or not to return the rotated variables.</li>
  <li><strong>center</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines if the variables should be shifted to be zero centered.</li>
  <li><strong>scale.</strong>: in Principal Component Analysis, variables are often scaled (i.e. standardized). This is particularly recommended when variables are measured in different scales (e.g: kilograms, kilometers, centimeters...); otherwise, the PCA outputs obtained will be severely affected. Generally, variables are scaled to have a standard deviation of 1 and a mean of 0, which is achieved by subtracting from each variable's element the variable's mean and then dividing the result by the variable' standard deviation.<br>
  Fact is scaling is useful under most scenarios and, to further empathize its relevance, every PCA's code snippet will showcase that the data is being scaled even if such is the default behavior of the associated argument.</li>
  <li><strong>tol</strong>: stands for tolerance; a value indicating the magnitude below which components should be omitted (defaults to <code>tol = <span class = "boolean">NULL</span></code> with which no components are omitted).</li>
  <li><strong>rank.</strong>: a number specifying the maximal rank, i.e., maximal number of principal components to be used.</li>
</ul>

More information regarding the <code>prcomp()</code> function and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp <br>

Let's detail the arguments for <code>princomp()</code>:
<ul>
  <li><strong>x</strong>: the numeric matrix or dataset/dataframe upon which to perform the PCA.</li>
  <li><strong>cor</strong>: a logical value indicating whether the calculation should use the correlation matrix or the covariance matrix.</li>
  <li><strong>scores</strong>: a logical value indicating whether the score on each principal component should be calculated.</li>
  <li><strong>covmat</strong>: an optional covariance matrix to be used rather than the covariance matrix of <code>x</code>.</li>
  <li><strong>subset</strong>: an optional vector used to select rows (observations) of the data matrix <code>x</code>.</li>
  <li><strong>fix_sign</strong>: a logical value indicating whether to choose the signs of the loadings and scores so that the first element of each loading is non-negative.</li>
</ul>

More information regarding the <code>princomp()</code> function and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/princomp <br>

Let's now evaluate the results when applying <code>prcomp()</code> to the Wisconsin Breast Cancer Dataset:

#### {.tabset}

##### All
```{r prcomp_1_1}
all_pca_1 <- prcomp(wbcd[,-1], scale. = TRUE)
class(all_pca_1)
str(all_pca_1)
summary(all_pca_1)
```

##### Mean
```{r prcomp_1_2}
mean_pca_1 <- prcomp(wbcd[,c(2:11)], scale. = TRUE)
class(mean_pca_1)
str(mean_pca_1)
summary(mean_pca_1)
```

##### SE
```{r prcomp_1_3}
se_pca_1 <- prcomp(wbcd[,c(12:21)], scale. = TRUE)
class(se_pca_1)
str(se_pca_1)
summary(se_pca_1)
```

##### Worst
```{r prcomp_1_4}
worst_pca_1 <- prcomp(wbcd[,c(22:31)], scale. = TRUE)
class(worst_pca_1)
str(worst_pca_1)
summary(worst_pca_1)
```

#### {-}
The function <code>class()</code> showcases that the object created by using the <code>prcomp()</code> function is a list of class "prcomp". Said object/list contains the following components, as illustrated by the function <code>str()</code>:
<ul>
  <li><strong>sdev</strong>: the standard deviations of the principal components (i.e., the square roots of the eigenvalues of the covariance/correlation matrix, though the calculation is actually done with the singular values of the data matrix).</li>
  <li><strong>rotation</strong>: the matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors); it is the equivalent of the <code>loadings</code> component of the <code>princomp()</code> function.</li>
  <li><strong>center</strong>: the centering used, if any, or <code class = "boolean">FALSE</code> otherwise.</li>
  <li><strong>scale</strong>: the scaling used, if any, or <code class = "boolean">FALSE</code> otherwise.</li>
  <li><strong>x</strong>: if the argument <code>retx</code> is <code class = "boolean">TRUE</code>, then this component holds the value of the rotated data in the form of the centered (and scaled if requested) data multiplied by the rotation matrix.</li>
</ul>

The results of applying to it the function <code>summary()</code> returns the standard deviation of each Principal Component as well as two core elements of the PCA: the proportion of variance and the cumulative proportion. The former indicates de percentage of the data explained by each Principal Component whereas the latter sums the proportion of variances of all the Principal Components up until the one being observed. Taking <code>all_pca_1</code> as an example, this means that first PC explains about 44.27% of the data, PC1 and PC2 cover 63.24% of the data, PC1~PC3 cover 72.64% of the data and so forth.<br>

Let's now repeat this exercise with the <code>princomp()</code> function and examine the results:

#### {.tabset}

##### All
```{r princomp_1_1}
all_pca_2 <- princomp(wbcd[,-1], cor = TRUE)
class(all_pca_2)
str(all_pca_2)
summary(all_pca_2)
```

##### Mean
```{r princomp_1_2}
mean_pca_2 <- princomp(wbcd[,c(2:11)], cor = TRUE)
class(mean_pca_2)
str(mean_pca_2)
summary(mean_pca_2)
```

##### SE
```{r princomp_1_3}
se_pca_2 <- princomp(wbcd[,c(12:21)], cor = TRUE)
class(se_pca_2)
str(se_pca_2)
summary(se_pca_2)
```

##### Worst
```{r princomp_1_4}
worst_pca_2 <- princomp(wbcd[,c(22:31)], cor = TRUE)
class(worst_pca_2)
str(worst_pca_2)
summary(worst_pca_2)
```

#### {-}
The function <code>class()</code> showcases that the object created by using the <code>princomp()</code> function is a list of class "princomp". Said object/list contains the following components, as illustrated by the function <code>str()</code>:
<ul>
  <li><strong>sdev</strong>: the standard deviations of the principal components.</li>
  <li><strong>loadings</strong>: the matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors); it is the equivalent of the <code>rotation</code> component of the <code>prcomp()</code> function.</li>
  <li><strong>center</strong>: the means that were subtracted.</li>
  <li><strong>scale</strong>: the scalings applied to each variable.</li>
  <li><strong>n.obs</strong>: the number of observations.</li>
  <li><strong>scores</strong>: if <code>scores = <span class = "boolean">TRUE</span></code> then this component holds the scores of the supplied data on the principal components.</li>
  <li><strong>call</strong>: the matched call.</li>
  <li><strong>na.action</strong>: if relevant.</li>
</ul>

As can be observed, the objects created by <code>prcomp()</code> and <code>princomp()</code> differ both in class and components.<br>

Once again, the results of applying to it the function <code>summary()</code> returns the standard deviation of each Principal Component as well as both the proportion of variance and the cumulative proportion. It is worth noting that the cumulative proportion with <code>princomp()</code> is identical to the one obtained with <code>prcomp()</code> - as should be.

### 2.2.2. PCA() from FactoMineR
The <code>PCA()</code> function from the <strong>FactoMineR</strong> package is formatted as follows:

```{r PCA_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(FactoMineR)
PCA(X, 
    scale.unit = TRUE, 
    ncp = 5,
    ind.sup = NULL, 
    quanti.sup = NULL, 
    quali.sup = NULL, 
    row.w = NULL, 
    col.w = NULL,
    graph = TRUE,
    axes = c(1,2)
    )
```

Let's detail its most notable arguments:
<ul>
  <li><strong>X</strong>: the numeric matrix or dataset/dataframe upon which to perform the PCA.</li>
  <li><strong>scale.unit</strong>: <code class = "boolean">TRUE</code> of <code class = "boolean">FALSE</code> determines whether or not to scale (i.e. standardize) the dataset/dataframe variables.</li>
  <li><strong>ncp</strong>: number of dimensions kept in the final results - for illustration purposes all the PCA functions will use as many dimensions (Principal Components) as original variables since such is the behavior of the functions covered until this point and, since Principal Components are sorted by relevance, one can always ignore the extra PCs later on (meaning that selecting a large-ish number at this point does not make much of a difference).</li>
  <li><strong>ind.sup</strong>: a vector indicating the indexes of the supplementary individuals.</li>
  <li><strong>quanti.sup</strong>: a vector indicating the indexes of the quantitative supplementary variables.</li>
  <li><strong>quali.sup</strong>: a vector indicating the indexes of the categorical supplementary variables.</li>
  <li><strong>row.w</strong>: an optional row weights.</li>
  <li><strong>col.w</strong>: an optional column weights.</li>
  <li><strong>graph</strong>: <code class = "boolean">TRUE</code> of <code class = "boolean">FALSE</code> determines whether or not to display the PCA's associated graph.</li>
  <li><strong>axes</strong>: a length 2 vector specifying the components to plot.</li>
</ul>

More information regarding the <code>PCA()</code> function and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/FactoMineR/versions/2.4/topics/PCA <br>

Let's now evaluate the results when applying <code>PCA()</code> to the Wisconsin Breast Cancer Dataset:

#### {.tabset}

##### All
```{r PCA_1_1}
all_pca_3 <- PCA(wbcd[,-1], scale.unit = TRUE, ncp = 30, graph = FALSE)
class(all_pca_3)
str(all_pca_3)
summary(all_pca_3)
```

##### Mean
```{r PCA_1_2}
mean_pca_3 <- PCA(wbcd[,c(2:11)], scale.unit = TRUE, ncp = 30, graph = FALSE)
class(mean_pca_3)
str(mean_pca_3)
summary(mean_pca_3)
```

##### SE
```{r PCA_1_3}
se_pca_3 <- PCA(wbcd[,c(12:21)], scale.unit = TRUE, ncp = 30, graph = FALSE)
class(se_pca_3)
str(se_pca_3)
summary(se_pca_3)
```

##### Worst
```{r PCA_1_4}
worst_pca_3 <- PCA(wbcd[,c(22:31)], scale.unit = TRUE, ncp = 30, graph = FALSE)
class(worst_pca_3)
str(worst_pca_3)
summary(worst_pca_3)
```

#### {-}
The function <code>class()</code> showcases that the object created by using the <code>PCA()</code> function is of class "list" and "PCA". It is also worth noting that the cumulative proportions obtained with <code>PCA()</code> (and seen when applying the <code>summary()</code> function to an object of this kind) are identical to the ones obtained with the R built-in functions - as should be.<br>

Another aspect to highlight about this function lies in how well organized is/are this function's output/results. Even though the components of the PCA object obtained through <code>PCA()</code> can be read in <code>str()</code>'s output (as was the case with R built-in functions), the function <code>print()</code> does a better job at showcasing the object's components and how the resulting data is stored within it/them - the following code snippets compare the outputs of the <code>print()</code> function when applied to the PCA objects described so far:

#### {.tabset}

##### prcomp()
``` {r print_pca_1}
print(all_pca_1)
```

##### princomp()
``` {r print_pca_2}
print(all_pca_2)
```

##### PCA() {.active}
``` {r print_pca_3}
print(all_pca_3)
```

#### {-}

Printing a "PCA" class object results in an organized look (moreso when compared to the alternatives) at the function's (object's) components.<br>

As was already stated, there are many aspects of PCA that are not covered by the scope of this documents due to time constraints and to keep the overall size within reasonable limits. <code>prcomp()</code> and <code>princomp()</code> components are not directly useful for the tasks that are yet to be performed, yet <code>PCA()</code>'s are - certain components (like the eigenvalues and the coordinates, correlations, squared cosines and contributions of both variables and individuals) are core to following procedures, albeit those will be detailed later on the document. For now, let's just state that <code>PCA()</code> is a more convenient function in most cases than both <code>prcomp()</code> and <code>princomp()</code> due to the these particular components (such information can be obtained with the other functions, but require the use of additional functions whereas <code>PCA()</code> makes such task more straightforward).

### 2.2.3. dudi.pca() from ade4
The <code>dudi.pca()</code> function from the <strong>ade4</strong> package is formatted as follows:

```{r dudi_pca_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(ade4)
dudi.pca(df,
         row.w = rep(1, nrow(df))/nrow(df),
         col.w = rep(1, ncol(df)),
         center = TRUE, 
         scale = TRUE,
         scannf = TRUE, 
         nf = 2,
         ...)
```

Let's detail its most notable arguments:
<ul>
  <li><strong>df</strong>: a data frame with n rows (individuals) and p columns (numeric variables).</li>
  <li><strong>row.w</strong>: an optional row weights.</li>
  <li><strong>col.w</strong>: an optional column weights.</li>
  <li><strong>center</strong>: <code class = "boolean">TRUE</code> of <code class = "boolean">FALSE</code> determines whether perform an optional row weight (by default, uniform row weights).</li>
  <li><strong>scale</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether or not to scale/standardize the data.</li>
  <li><strong>scannf</strong>: <code class = "boolean">TRUE</code> of <code class = "boolean">FALSE</code> determines whether or not to display an screeplot (the topic is properly explained later on the document).</li>
  <li><strong>nf</strong>: an integer to indicate the number of kept axes if <code>scannf = <span class = "boolean">FALSE</span></code></li>
</ul>

More information regarding the <code>dudi.pca()</code> function and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/ade4/versions/1.7-15/topics/dudi.pca <br>

Let's now evaluate the results when applying <code>dudi.pca()</code> to the Wisconsin Breast Cancer Dataset:

#### {.tabset}

##### All
```{r dudi_pca_1_1}
all_pca_4 <- dudi.pca(wbcd[,-1], scale = TRUE, scannf = FALSE, nf = 30)
class(all_pca_4)
str(all_pca_4)
summary(all_pca_4)
```

##### Mean
```{r dudi_pca_1_2}
mean_pca_4 <- dudi.pca(wbcd[,c(2:11)], scale = TRUE, scannf = FALSE, nf = 30)
class(mean_pca_4)
str(mean_pca_4)
summary(mean_pca_4)
```

##### SE
```{r dudi_pca_1_3}
se_pca_4 <- dudi.pca(wbcd[,c(12:21)], scale = TRUE, scannf = FALSE, nf = 30)
class(se_pca_4)
str(se_pca_4)
summary(se_pca_4)
```

##### Worst
```{r dudi_pca_1_4}
worst_pca_4 <- dudi.pca(wbcd[,c(22:31)], scale = TRUE, scannf = FALSE, nf = 30)
class(worst_pca_4)
str(worst_pca_4)
summary(worst_pca_4)
```

#### {-}

The function <code>class()</code> showcases that the object created by using the <code>dudi.pca()</code> function is of class "pca" (not "PCA" like <code>PCA()</code>'s objects) and "dudi". It is also worth noting that the cumulative proportions obtained with <code>dudi.pca()</code> (and seen when applying the <code>summary()</code> function to an object of this kind) are identical to the ones obtained with the PCA functions covered up until this point - as should be.<br>

Once again, even though the components of the PCA object obtained through <code>dudi.pca()</code> can be read in <code>str()</code>'s output, the function <code>print()</code> does a better job at showcasing the object's components and how the resulting data is stored within it/them.

``` {r print_pca_4}
print(all_pca_4)
```

The resulting output isn't as impressive as was with <code>PCA()</code>'s objects, but it's a cleaner way to observe the components than through <code>str()</code> and definitely an improvement over R built-in functions. These components are the following:
<ul>
  <li><strong>tab</strong>: the data frame to be analyzed depending of the transformation arguments (center and scale).</li>
  <li><strong>cw</strong>: the column weights.</li>
  <li><strong>lw</strong>: the row weights.</li>
  <li><strong>eig</strong>: the eigenvalues.</li>
  <li><strong>rank</strong>: the rank of the analyzed matrice.</li>
  <li><strong>nf</strong>: the number of kept factors.</li>
  <li><strong>c1</strong>: the column normed scores i.e. the principal axes.</li>
  <li><strong>l1</strong>: the row normed scores.</li>
  <li><strong>co</strong>: the column coordinates.</li>
  <li><strong>li</strong>: the row coordinates i.e. the principal components.</li>
  <li><strong>call</strong>: the call function.</li>
  <li><strong>cent</strong>: the <code>p</code> vector containing the means for variables.</li>
  <li><strong>norm</strong>: the <code>p</code> vector containing the standard deviations for variables i.e. the root of the sum of squares deviations of the values from their means divided by <code>n</code></li>
</ul>

Having the eigenvalues stored as a component is an improvement with respect to R built-in functions, but <code>PCA()</code>'s arguments are more convenient under most circumstances.

### 2.2.4. epPCA() from ExPosition
The <code>epPCA()</code> function from the <strong>ExPosition</strong> package is formatted as follows:

```{r epPCA_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(ExPosition)
epPCA(DATA, 
      scale = TRUE, 
      center = TRUE, 
      DESIGN = NULL, 
      make_design_nominal = TRUE, 
      graphs = TRUE, 
      k = 0)
```

Let's detail its arguments:
<ul>
  <li><strong>DATA</strong>: the numeric matrix or dataset/dataframe upon which to perform the PCA.</li>
  <li><strong>scale</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether or not to scale/standardize the data.</li>
  <li><strong>center</strong>: <code class = "boolean">TRUE</code> of <code class = "boolean">FALSE</code> determines whether perform an optional row weight (by default, uniform row weights).</li>
  <li><strong>DESIGN</strong>: a design matrix to indicate if rows belong to groups.</li>
  <li><strong>make_design_nominal</strong>: if <code class = "boolean">TRUE</code> (default) then <code>DESIGN</code> is a vector that indicates groups (and will be dummy-coded); if <code class = "boolean">FALSE</code> then <code>DESIGN</code> is a dummy-coded matrix.</li>
  <li><strong>graph</strong>: <code class = "boolean">TRUE</code> of <code class = "boolean">FALSE</code> determines whether or not to display the PCA's associated graph.</li>
  <li><strong>k</strong>: number of components to return.</li>
</ul>

More information regarding the <code>epPCA()</code> function and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/ExPosition/versions/2.8.23/topics/epPCA <br>

Let's now evaluate the results when applying <code>epPCA()</code> to the Wisconsin Breast Cancer Dataset:

#### {.tabset}

##### All
```{r epPCA_1_1}
all_pca_5 <- epPCA(wbcd[,-1], scale = TRUE, graphs = FALSE, k = 30)
class(all_pca_5)
str(all_pca_5)
summary(all_pca_5)
```

##### Mean
```{r epPCA_1_2}
mean_pca_5 <- epPCA(wbcd[,c(2:11)], scale = TRUE, graphs = FALSE, k = 30)
class(mean_pca_5)
str(mean_pca_5)
summary(mean_pca_5)
```

##### SE
```{r epPCA_1_3}
se_pca_5 <- epPCA(wbcd[,c(12:21)], scale = TRUE, graphs = FALSE, k = 30)
class(se_pca_5)
str(se_pca_5)
summary(se_pca_5)
```

##### Worst
```{r epPCA_1_4}
worst_pca_5 <- epPCA(wbcd[,c(22:31)], scale = TRUE, graphs = FALSE, k = 30)
class(worst_pca_5)
str(worst_pca_5)
summary(worst_pca_5)
```

#### {-}

The function <code>class()</code> showcases that the object created by using the <code>epPCA()</code> function is of class "list" and "expoOutput". It is also worth noting that the cumulative proportions easily observed with previous functions is hidden somewhere within the object and not readily available for an evaluation.<br>

The output of both <code>str()</code> and <code>summary()</code> suggest that the core components are hidden within two "father" components by the name of <code>ExPosition.Data</code> and <code>Plotting.Data</code>. The output of applying the <code>print()</code> function to the object at hand further evidentiates this:

``` {r print_pca_5}
print(all_pca_5)
```

The brief descriptions given by <code>print()</code>'s output imply that the components equivalent to those seen in previous functions are stored inside <code>ExPosition.Data</code> whereas <code>Plotting.Data</code> holds data regarding an optional plot (as their name would suggest).<br>

The following code snippet details the components stored inside <code>ExPosition.Data</code>:

#### {.tabset}

##### All
```{r epPCA_Data_1_1}
print(all_pca_5$ExPosition.Data)
summary(all_pca_5$ExPosition.Data)
```

##### Mean
```{r epPCA_Data_1_2}
print(mean_pca_5$ExPosition.Data)
summary(mean_pca_5$ExPosition.Data)
```

##### SE
```{r epPCA_Data_1_3}
print(se_pca_5$ExPosition.Data)
summary(se_pca_5$ExPosition.Data)
```

##### Worst
```{r epPCA_Data_1_4}
print(worst_pca_5$ExPosition.Data)
summary(worst_pca_5$ExPosition.Data)
```

#### {-}

As was the case with <code>dudi.pca()</code>, having the eigenvalues stored as a component is an improvement with respect to R built-in functions, but <code>PCA()</code>'s arguments are more convenient under most circumstances. What's more: this is the only PCA function with no direct access to cumulative variance.

### 2.2.5. Eigenvalues
The aforementioned video essay by Josh Starmer (https://www.youtube.com/watch?v=FgakZw6K1QQ) helps with the understanding of the <strong>eigenvalues</strong> (also known as singular vectors) since there is a graphical explanation detailing where they come from, but in summary they are a unit long vector whose slope is given by the amount of variation retained by each principal component. As such, eigenvalues store said variation and are therefore large for the first PCs and small for the subsequent ones, meaning that the first PCs correspond to the directions with the maximum amount of variation in the dataset (which makes sense since the first PC is the one that better explains the dataset).<br>

Some of the previously described functions made the eigenvalues easily available and accessible - doing so requires addressing the proper component within each of said functions:

#### {.tabset}

##### PCA()
``` {r eigenvalue_1_1}
all_pca_3$eig
```

##### dudi.pca()
``` {r eigenvalue_1_2}
all_pca_4$eig
```

##### epPCA()
``` {r eigenvalue_1_3}
all_pca_5$ExPosition.Data$eigs
```

#### {-}

Note that, for example, accessing the eigenvalues of the objects created through the use of the <code>dudi.pca</code> and <code>epPCA()</code> functions does not return the variances associated with each of the eigenvalues, which is an useful metric one would rather observe and evaluate than not. In <code>epPCA()</code>'s case these variances can be accessed via <code><span>&#36;</span>ExPosition.Data<span>&#36;</span>t</code>, but as was already stated there is not a component for cumulative variances. In <code>dudi.pca()</code>'s case, one can observe these variances through the <code>summary()</code> function, but format-wise that is less than ideal. Fortunately, there are alternative ways to obtain the eigenvalues - the package <strong>factoextra</strong> includes several functions to extract and visualize these variances. More information about them can be found in their associated RDocumentation page: https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/eigenvalue <br>

Let's focus on the eigenvalues themselves, which can be extracted through the functions <code>get_eig()</code> and <code>get_eigenvalue()</code>. Note that both functions are identical, one is but an alias of the other.

``` {r get_eigenvalue_0, eval = FALSE}
library(factoextra)
get_eig(pca)
get_eigenvalue(pca)
```

Let's observe the results of applying said functions to the previously constructed PCAs:

#### {.tabset}

##### prcomp()
``` {r get_eigenvalue_1_1}
get_eig(all_pca_1)
get_eigenvalue(all_pca_1)
identical(get_eig(all_pca_1), get_eigenvalue(all_pca_1))
```

##### princomp()
``` {r get_eigenvalue_1_2}
get_eig(all_pca_2)
get_eigenvalue(all_pca_2)
identical(get_eig(all_pca_2), get_eigenvalue(all_pca_2))
```

##### PCA()
``` {r get_eigenvalue_1_3}
get_eig(all_pca_3)
get_eigenvalue(all_pca_3)
identical(get_eig(all_pca_3), get_eigenvalue(all_pca_3))
```

##### dudi.pca()
``` {r get_eigenvalue_1_4}
get_eig(all_pca_4)
get_eigenvalue(all_pca_4)
identical(get_eig(all_pca_4), get_eigenvalue(all_pca_4))
```

##### epPCA()
``` {r get_eigenvalue_1_5}
get_eig(all_pca_5)
get_eigenvalue(all_pca_5)
identical(get_eig(all_pca_5), get_eigenvalue(all_pca_5))
```

#### {-}

Every single PCA object returns the same eigenvalues and variances, as should be. Is also worth noting that both <code>get_eig()</code> and <code>get_eigenvalue()</code> yield the same results - the function <code>identical()</code> returns <code class = "boolean">TRUE</code> in each scenario, indicating equality between both functions' outputs.<br>

Despite the usefulness of having the eigenvalues and variances in a tidy dataframe such as the ones obtained in the previous code snippets, being able to visualize the differences graphically helps to illustrate it all - that is when the screeplot comes into play.

### 2.2.6. Screeplot
The RDocumentation page detailing both both <code>get_eig()</code> and <code>get_eigenvalue()</code> also mentions the <code>fviz_eig()</code> and the <code>fviz_screeplot()</code> functions which, as was the case with the previous functions, are identical in behavior - one is but an alias of the other.<br>

These functions draw what is know as a "screeplot" or "scree plot", which is a graph of eigenvalues ordered from largest to smallest. It can also be interpreted as a plot of the percentage of variance associated with each Principal Component or dimension (since such is the definition of an eigenvalue after all).<br>

Unlike the previous <strong>factoextra</strong> functions, these ones accept multiple arguments in order to customize the screeplot:
<ul>
  <li><strong>X</strong>: an object obtained through any of the PCA functions previously detailed.</li>
  <li><strong>choice</strong>: a text specifying the data to be plotted. Allowed values are <code>"variance"</code> or <code>"eigenvalue"</code>.</li>
  <li><strong>geom</strong>: a text specifying the geometry to be used for the graph. Allowed values are <code>"bar"</code> for barplot, <code>"line"</code> for lineplot or <code>c("bar", "line")</code> to use both types.</li>
  <li><strong>barfill</strong>: fill color for bar plot.</li>
  <li><strong>barcolor</strong>: outline color for bar plot.</li>
  <li><strong>linecolor</strong>: color for line plot.</li>
  <li><strong>ncp</strong>: a numeric value specifying the number of dimensions to be shown.</li>
  <li><strong>addlabels</strong>: <code class = "boolean">TRUE</code> of <code class = "boolean">FALSE</code> determines whether or not labels are added at the top of bars or points showcasing the information retained by each dimension.</li>
  <li><strong>hjust</strong>: horizontal adjustment of the labels.</li>
  <li><strong>main, xlab, ylab</strong>: plot main and axis titles.</li>
  <li><strong>ggtheme</strong>: allows the user to set a ggplot2 theme.</li>
</ul>

``` {r screeplot_0, eval = FALSE}
library(factoextra)
fviz_eig(X,
         choice = "variance",
         geom = c("bar", "line"),
         barfill = "steelblue",
         barcolor = "steelblue",
         linecolor = "black",
         ncp = 10,
         addlabels = FALSE,
         hjust = 0,
         main = NULL,
         xlab = NULL,
         ylab = NULL,
         ggtheme = theme_minimal(),
         ...)
fviz_screeplot(pca, ...)
```

These functions also accept optional arguments to be passed onto the function <code>ggpar()</code> upon which they are based. More information regarding these functions and their arguments is available at the very same RDocumentation page as the functions <code>get_eig()</code> and <code>get_eigenvalue()</code> (https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/eigenvalue).<br>

Let's now observe the results of applying said functions to the previously constructed PCA objects:

#### {.tabset}

##### prcomp()
``` {r screeplot_1_1}
fviz_eig(all_pca_1, 
         addlabels = TRUE, 
         ylim = c(0, 50), 
         barfill = "#81d4fa", 
         barcolor = "#81d4fa", 
         linecolor = "red")
```

##### princomp()
``` {r screeplot_1_2}
fviz_eig(all_pca_2, 
         addlabels = TRUE, 
         ylim = c(0, 50), 
         barfill = "#81d4fa", 
         barcolor = "#81d4fa", 
         linecolor = "red")
```

##### PCA()
``` {r screeplot_1_3}
fviz_eig(all_pca_3, 
         addlabels = TRUE, 
         ylim = c(0, 50), 
         barfill = "#81d4fa", 
         barcolor = "#81d4fa", 
         linecolor = "red")
```

##### dudi.pca()
``` {r screeplot_1_4}
fviz_eig(all_pca_4, 
         addlabels = TRUE, 
         ylim = c(0, 50), 
         barfill = "#81d4fa", 
         barcolor = "#81d4fa", 
         linecolor = "red")
```

##### epPCA()
``` {r screeplot_1_5}
fviz_eig(all_pca_5, 
         addlabels = TRUE, 
         ylim = c(0, 50), 
         barfill = "#81d4fa", 
         barcolor = "#81d4fa", 
         linecolor = "red")
```

#### {-}

Unsurprisingly, all of the screeplots are identical.<br>

As a side note, the <code>scannf</code> argument of the function <code>dudi.pca()</code> allows for a screeplot to be plotted. Setting said argument to <code class = "boolean">TRUE</code> would yield a plot akin to the ones obtained earlier with the function <code>fviz_eig()</code>, albeit considerably less complete/polished.<br>

These plots help to visualize the variance results obtained when performing a PCA upon the Wisconsin Breast Cancer Dataset. Unfortunately, there is no well-accepted objective way to decide how many Principal Components are enough - this will depend on the specific field of application and the specific dataset (biomedical scenarios tend to require high cummulative variance since people's health is at play). In practice, the first few principal components are the most important ones in order to find interesting patterns in the data and undoubtedly the most important ones when it comes to representing the data.

### 2.2.7. Variables and individuals
Navigating through the PCA outputs previously coded is not an easy task. As was the case with the eigenvalues, some of the PCA functions yield an object whose components include the results for variables and individuals, namely <code>PCA()</code> and <code>epPCA()</code>. 

#### {.tabset}

##### PCA()
``` {r pca_var_1_1}
print(all_pca_3)
```

##### epPCA()
``` {r pca_var_1_2}
print(all_pca_5$ExPosition.Data)
```

#### {-}

In <code>PCA()</code>'s case, the results for variables and individuals are accessed with <code><span>&#36;</span>var</code> and <code><span>&#36;</span>ind</code> respectively, whereas <code>epPCA()</code> has a unique address for every result (less ideal, but manageable).<br>

A simpler method to extract the results for variables and individuals from a PCA output is to use the function <code>get_pca_var()</code> and <code>get_pca_ind()</code> respectively. There's also the option of using the function <code>get_pca()</code> with the argument <code>element = "var"</code> for the results for variables or with the argument <code>element = "ind"</code> for the results for individuals.<br>

All of these functions come from the <strong>factoextra</strong> package and provide a list of matrices containing all the results for either the active variables or individuals (more information regarding these functions is available in their associated RDocumentation page: https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/get_pca).<br>

Let's first examine the outputs of applying <code>get_pca_var()</code> to the PCA objects previously created:

#### {.tabset}

##### prcomp()
``` {r get_pca_var_1_1}
all_pca_var_1 <- get_pca_var(all_pca_1)
all_pca_var_1
head(all_pca_var_1$coord)
head(all_pca_var_1$cor)
head(all_pca_var_1$cos2)
head(all_pca_var_1$contrib)
```

##### princomp()
``` {r get_pca_var_1_2}
all_pca_var_2 <- get_pca_var(all_pca_2)
all_pca_var_2
head(all_pca_var_2$coord)
head(all_pca_var_2$cor)
head(all_pca_var_2$cos2)
head(all_pca_var_2$contrib)
```

##### PCA()
``` {r get_pca_var_1_3}
all_pca_var_3 <- get_pca_var(all_pca_3)
all_pca_var_3
head(all_pca_var_3$coord)
head(all_pca_var_3$cor)
head(all_pca_var_3$cos2)
head(all_pca_var_3$contrib)
```

##### dudi.pca()
``` {r get_pca_var_1_4}
all_pca_var_4 <- get_pca_var(all_pca_4)
all_pca_var_4
head(all_pca_var_4$coord)
head(all_pca_var_4$cor)
head(all_pca_var_4$cos2)
head(all_pca_var_4$contrib)
```

##### epPCA()
``` {r get_pca_var_1_5}
all_pca_var_5 <- get_pca_var(all_pca_5)
all_pca_var_5
head(all_pca_var_5$coord)
head(all_pca_var_5$cor)
head(all_pca_var_5$cos2)
head(all_pca_var_5$contrib)
```

#### {-}

Unsurprisingly, each object yields the same output. Let's now examine the outputs of applying <code>get_pca_ind()</code> to the PCA objects previously created:

#### {.tabset}

##### prcomp()
``` {r get_pca_ind_1_1}
all_pca_ind_1 <- get_pca_ind(all_pca_1)
all_pca_ind_1
head(all_pca_ind_1$coord)
head(all_pca_ind_1$cos2)
head(all_pca_ind_1$contrib)
```

##### princomp()
``` {r get_pca_ind_1_2}
all_pca_ind_2 <- get_pca_ind(all_pca_2)
all_pca_ind_2
head(all_pca_ind_2$coord)
head(all_pca_ind_2$cos2)
head(all_pca_ind_2$contrib)
```

##### PCA()
``` {r get_pca_ind_1_3}
all_pca_ind_3 <- get_pca_ind(all_pca_3)
all_pca_ind_3
head(all_pca_ind_3$coord)
head(all_pca_ind_3$cos2)
head(all_pca_ind_3$contrib)
```

##### dudi.pca()
``` {r get_pca_ind_1_4}
all_pca_ind_4 <- get_pca_ind(all_pca_4)
all_pca_ind_4
head(all_pca_ind_4$coord)
head(all_pca_ind_4$cos2)
head(all_pca_ind_4$contrib)
```

##### epPCA()
``` {r get_pca_ind_1_5}
all_pca_ind_5 <- get_pca_ind(all_pca_5)
all_pca_ind_5
head(all_pca_ind_5$coord)
head(all_pca_ind_5$cos2)
head(all_pca_ind_5$contrib)
```

#### {-}

Each of these results has its usefulness, although more often than not a plot is required in order to properly interpret the information they hold. Some of these plots and applications are overviewed from this point onward.

### 2.2.8. Contributions
It was already stated that Principal Components are linear combinations of the dataset original features/variables. These linear combinations are more dependant on certain variables than upon others, and the distribution is stored within the results for variables under the <code><span>&#36;</span>contrib</code> index and expressed as a percentage (as in how much of a given Principal Component is determined by the variable in question). Note that each of the individuals also contributes to the Principal Components (albeit in a less direct way due to the higher amount of them) - these individual contributions are stored within the results for individuals under the <code><span>&#36;</span>contrib</code> index (also expressed as a percentage).<br>

The function <code>fviz_contrib()</code> from the <strong>factoextra</strong> package can be used to draw a barplot of these contributions.

``` {r fviz_contrib_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(factoextra)
fviz_contrib(X,
             choice = c("row", "col", "var", "ind", "quanti.var", "quali.var",
                        "group", "partial.axes"),
             axes = 1,
             fill = "steelblue",
             color = "steelblue",
             sort.val = c("desc", "asc", "none"),
             top = Inf,
             ggtheme = theme_minimal(),
             ...)
```

Let's detail the function's arguments:
<ul>
  <li><strong>X</strong>: an object of class <code>PCA</code>, <code>CA</code>, <code>MCA</code>, <code>FAMD</code>, <code>MFA</code> and <code>HMFA</code> (from the <strong>FactoMineR</strong> package); <code>prcomp</code> and <code>princomp</code> (from R built-in functions); <code>dudi</code>, <code>pca</code>, <code>coa</code> and <code>acm</code> (from the <strong>ade4</strong> package); <code>ca</code> (from the <strong>ca</strong> package).</li>
  <li><strong>choice</strong>: allowed values are <code>"row"</code> and <code>"col"</code> for <code>CA</code> objects; <code>"var"</code> and <code>"ind"</code> for <code>PCA</code> or <code>MCA</code> objects; <code>"var"</code>, <code>"ind"</code>, <code>"quanti.var"</code>, <code>"quali.var"</code> and <code>"group"</code> for FAMD, MFA and HMFA objects.</li>
  <li><strong>axes</strong>: a numeric vector specifying the dimension(s) of interest (it can be used to evaluate contributions either in a single dimension or across multiple dimensions, as is showcased in the upcoming code snippet).</li>
  <li><strong>fill</strong>: a fill color for the bar plot.</li>
  <li><strong>color</strong>: an outline color for the bar plot.</li>
  <li><strong>sort.val</strong>: a string specifying whether the value should be sorted. Allowed values are <code>"none"</code> (no sorting), <code>"asc"</code> (for ascending) or <code>"desc"</code> (for descending).</li>
  <li><strong>top</strong>: a numeric value specifying the number of top elements to be shown.</li>
  <li><strong>ggtheme</strong>: allows the user to tweak the plot's aesthetic through a ggplot2-based theme customization.</li>
</ul>

More information regarding the <code>fviz_contrib()</code> function and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_contrib <br>

Let's now observe the results of applying the function at hand to the previously constructed PCA objects. Note that various graphs are plotted to showcase an evalution of these contributions in various scenarios: a pair of single-dimensional ones and a combination of these (a multi-dimension scenario) - to do so, the function <code>grid.arrange()</code> from the <strong>gridExtra</strong> package is used; more information regarding this function and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/gridExtra/versions/2.3/topics/arrangeGrob <br>

The following code snippets and plots cover the contribution of variables to PCs:

#### {.tabset}

##### prcomp()
``` {r fviz_contrib_1_1, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_1, 
                          choice = "var", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_1, 
                          choice = "var", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_1, 
                          choice = "var", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### princomp()
``` {r fviz_contrib_1_2, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_2, 
                          choice = "var", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_2, 
                          choice = "var", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_2, 
                          choice = "var", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### PCA()
``` {r fviz_contrib_1_3, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_3, 
                          choice = "var", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_3, 
                          choice = "var", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_3, 
                          choice = "var", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### dudi.pca()
``` {r fviz_contrib_1_4, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_4, 
                          choice = "var", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_4, 
                          choice = "var", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_4, 
                          choice = "var", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### epPCA()
``` {r fviz_contrib_1_5, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_5, 
                          choice = "var", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_5, 
                          choice = "var", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_5, 
                          choice = "var", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

#### {-}

<br>

Unsurprisingly, all of the resulting plots are identical - as should be. Let's now evaluate the contribution of individuals to PCs:<br>

#### {.tabset}

##### prcomp()
``` {r fviz_contrib_1_6, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_1, 
                          choice = "ind", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_1, 
                          choice = "ind", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_1, 
                          choice = "ind", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### princomp()
``` {r fviz_contrib_1_7, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_2, 
                          choice = "ind", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_2, 
                          choice = "ind", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_2, 
                          choice = "ind", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### PCA()
``` {r fviz_contrib_1_8, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_3, 
                          choice = "ind", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_3, 
                          choice = "ind", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_3, 
                          choice = "ind", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### dudi.pca()
``` {r fviz_contrib_1_9, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_4, 
                          choice = "ind", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_4, 
                          choice = "ind", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_4, 
                          choice = "ind", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### epPCA()
``` {r fviz_contrib_1_10, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_contrib(all_pca_5, 
                          choice = "ind", 
                          axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_contrib(all_pca_5, 
                          choice = "ind", 
                          axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_contrib(all_pca_5, 
                          choice = "ind", 
                          axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

#### {-}

<br>

Once again, identical outputs - as should be. Note that the amount of individuals makes it impossible to apreciate the X-axis' labels (without an extreme zoom-in). A more illustrative approach to visualizing these contributions involves the use of the <code>corrplot()</code> function from the <strong>corrplot</strong> package, which showcases every contribution (either variables' or individuals') on every Principal Component.

``` {r corrplot_contrib_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(corrplot)
corrplot(corr, 
         is.corr = FALSE,
         ...)
```

<code>corrplot()</code> is a really complete function with much attention to detail, meaning that there are way too many arguments to tweak its behavior and graph for this document to cover - given that most of them are of negligible relevance for the task at hand, only the key arguments will be detailed:
<ul>
  <li><strong>corr</strong>: the correlation matrix to visualize.</li>
  <li><strong>method</strong>: the visualization method of correlation matrix to be used. It currently supports seven methods, named <code>'circle'</code> (default), <code>'square'</code>, <code>'ellipse'</code>, <code>'number'</code>, <code>'pie'</code>, <code>'shade'</code> and <code>'color'</code>.</li>
  <li><strong>type</strong>: a character determines whether to display full matrix (<code>'full'</code>), lower triangular (<code>'lower'</code>) or upper triangular matrix (<code>'upper'</code>).</li>
  <li><strong>add</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines wheter or not to add the graph to an existing plot or not.</li>
  <li><strong>title</strong>: the title of the graph</li>
  <li><strong>is.corr</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether the input matrix is a correlation matrix or not.</li>
  <li><strong>diag</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to display the correlation coefficient on the principal diagonal</li>
</ul>

More information regarding the <code>corrplot()</code> function and all of its arguments is available in its associated RDocumentation page: page: https://www.rdocumentation.org/packages/corrplot/versions/0.90/topics/corrplot <br>

Let's now observe the results of applying the function at hand to the Wisconsin Breast Cancer Dataset. Note that the function's very first argument corresponds to the contributions themselves (previous functions used the PCA objects instead) - let's evaluate the variables first:

#### {.tabset}

##### prcomp()
``` {r corrplot_contrib_1_1}
corrplot(all_pca_var_1$contrib, is.corr=FALSE)
```

##### princomp()
``` {r corrplot_contrib_1_2}
corrplot(all_pca_var_2$contrib, is.corr=FALSE)
```

##### PCA()
``` {r corrplot_contrib_1_3}
corrplot(all_pca_var_3$contrib, is.corr=FALSE)
```

##### dudi.pca()
``` {r corrplot_contrib_1_4}
corrplot(as.matrix(all_pca_var_4$contrib), is.corr=FALSE)
```

##### epPCA()
``` {r corrplot_contrib_1_5}
corrplot(all_pca_var_5$contrib, is.corr=FALSE)
```

#### {-}

Once again, all of the resulting plots are identical - as should be. However, note the use of the function <code>as.matrix()</code> within the <code>dudi.pca()</code> tab - that is due to the particular structure of said function's objects which creates oddities when applying certain functions such as <code>get_pca_var()</code> or <code>get_pca_ind()</code>. The resulting objects obtained via said functions are usually of <code>matrix</code> class, but in this case it is a <code>data.frame</code> instead; using <code>as.matrix()</code> reformats the <code>data.frame</code> so that the function <code>corrplot()</code> accepts the input.<br>

Applying <code>corrplot()</code> upon the individuals' contributions would yield a massive plot due to the sheer amount of individuals. Given that, the following code snippets are not rendered in order to keep the document clean and readable.

#### {.tabset}

##### prcomp()
``` {r corrplot_contrib_1_6, eval = FALSE}
corrplot(all_pca_ind_1$contrib, is.corr=FALSE)
```

##### princomp()
``` {r corrplot_contrib_1_7, eval = FALSE}
corrplot(all_pca_ind_2$contrib, is.corr=FALSE)
```

##### PCA()
``` {r corrplot_contrib_1_8, eval = FALSE}
corrplot(all_pca_ind_3$contrib, is.corr=FALSE)
```

##### dudi.pca()
``` {r corrplot_contrib_1_9, eval = FALSE}
corrplot(as.matrix(all_pca_ind_4$contrib), is.corr=FALSE)
```

##### epPCA()
``` {r corrplot_contrib_1_10, eval = FALSE}
corrplot(all_pca_ind_5$contrib, is.corr=FALSE)
```

#### {-}

### 2.2.9. Quality of representation
The quality of representation (cos2) measures how well represented is a given variable within a given Principal Component (or within a set of them). This logic is also applied to the individuals, meaning that cos2 measures how well represented they are within a given Principal Component (or within a set of them). It is worth noting that for any given variable or individual the sum of the cos2 across all the Principal Components is equal to one.<br>

The function <code>fviz_cos2()</code> from the <strong>factoextra</strong> package helps to visualize through a barplot which of the PCA variables and/or individuals are best represented within a certain Principal Component (or within a set of Principal Components).

``` {r fviz_cos2_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(factoextra)
fviz_cos2(X,
          choice = c("row", "col", "var", "ind", "quanti.var", "quali.var", "group"),
          axes = 1,
          fill = "steelblue",
          color = "steelblue",
          sort.val = c("desc", "asc", "none"),
          top = Inf,
          xtickslab.rt = 45,
          ggtheme = theme_minimal(),
          ...)
```

Let's detail the function's arguments:
<ul>
  <li><strong>X</strong>: an object of class <code>PCA</code>, <code>CA</code>, <code>MCA</code>, <code>FAMD</code>, <code>MFA</code> and <code>HMFA</code> (from the <strong>FactoMineR</strong> package); <code>prcomp</code> and <code>princomp</code> (from R built-in functions); <code>dudi</code>, <code>pca</code>, <code>coa</code> and <code>acm</code> (from the <strong>ade4</strong> package); <code>ca</code> (from the <strong>ca</strong> package).</li>
  <li><strong>choice</strong>: allowed values are <code>"row"</code> and <code>"col"</code> for <code>CA</code> objects; <code>"var"</code> and <code>"ind"</code> for <code>PCA</code> or <code>MCA</code> objects; <code>"var"</code>, <code>"ind"</code>, <code>"quanti.var"</code>, <code>"quali.var"</code> and <code>"group"</code> for FAMD, MFA and HMFA objects.</li>
  <li><strong>axes</strong>: a numeric vector specifying the dimension(s) of interest (it can be used to evaluate cos2 either in a single dimension or across multiple dimensions, as is showcased in the upcoming code snippets).</li>
  <li><strong>fill</strong>: a fill color for the bar plot.</li>
  <li><strong>color</strong>: an outline color for the bar plot.</li>
  <li><strong>sort.val</strong>: a string specifying whether the value should be sorted. Allowed values are <code>"none"</code> (no sorting), <code>"asc"</code> (for ascending) or <code>"desc"</code> (for descending).</li>
  <li><strong>top</strong>: a numeric value specifying the number of top elements to be shown.</li>
  <li><strong>ggtheme</strong>: allows the user to tweak the plot's aesthetic through a ggplot2-based theme customization.</li>
</ul>

More information regarding the <code>fviz_cos2()</code> function and its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_cos2 <br>

Let's now observe the results of applying the function at hand to the previously constructed PCA objects. Note that various graphs are plotted to showcase an evalution of cos2 in various scenarios: a pair of single-dimensional ones and a combination of these (a multi-dimension scenario) - to do so, the function <code>grid.arrange()</code> from the <strong>gridExtra</strong> package is used once again.<br>

The following code snippets and plots cover the contribution of variables to PCs:

#### {.tabset}

##### prcomp()
``` {r fviz_cos2_1_1, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_1, 
                       choice = "var", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_1, 
                       choice = "var", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_1, 
                       choice = "var", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### princomp()
``` {r fviz_cos2_1_2, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_2, 
                       choice = "var", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_2, 
                       choice = "var", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_2, 
                       choice = "var", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### PCA()
``` {r fviz_cos2_1_3, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_3, 
                       choice = "var", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_3, 
                       choice = "var", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_3, 
                       choice = "var", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### dudi.pca()
``` {r fviz_cos2_1_4, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_4, 
                       choice = "var", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_4, 
                       choice = "var", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_4, 
                       choice = "var", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### epPCA()
``` {r fviz_cos2_1_5, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_5, 
                       choice = "var", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_5, 
                       choice = "var", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_5, 
                       choice = "var", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

#### {-}

<br>

Unsurprisingly, all of the resulting plots are identical - as should be. Let's now evaluate the cos2 for the individuals:<br>

#### {.tabset}

##### prcomp()
``` {r fviz_cos2_1_6, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_1, 
                       choice = "ind", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_1, 
                       choice = "ind", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_1, 
                       choice = "ind", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### princomp()
``` {r fviz_cos2_1_7, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_2, 
                       choice = "ind", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_2, 
                       choice = "ind", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_2, 
                       choice = "ind", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### PCA()
``` {r fviz_cos2_1_8, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_3, 
                       choice = "ind", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_3, 
                       choice = "ind", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_3, 
                       choice = "ind", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### dudi.pca()
``` {r fviz_cos2_1_9, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_4, 
                       choice = "ind", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_4, 
                       choice = "ind", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_4, 
                       choice = "ind", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

##### epPCA()
``` {r fviz_cos2_1_10, fig.height = 12, fig.width = 14}
barplot_1 <- fviz_cos2(all_pca_5, 
                       choice = "ind", 
                       axes = 1) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_2 <- fviz_cos2(all_pca_5, 
                       choice = "ind", 
                       axes = 2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

barplot_3 <- fviz_cos2(all_pca_5, 
                       choice = "ind", 
                       axes = 1:2) + 
  theme(text = element_text(size = 10),
        axis.text.x = element_text(angle = 90))

grid.arrange(barplot_1, barplot_2, barplot_3, nrow = 3, ncol = 1)
```

#### {-}

Once again, identical outputs - as should be. Note that, once again, the amount of individuals makes it impossible to apreciate the X-axis' labels (without an extreme zoom-in).<br>

As was already stated, a more illustrative approach to visualizing a data array such as the contributions' one or this one (with the quality of representations for both variables and individuals) can be achieved through the use of the <code>corrplot()</code> function from the <strong>corrplot</strong> package, which in this case showcases the quality of representation of every variable or individual within every Principal Component.<br>

Let's now observe the results of applying said function to the Wisconsin Breast Cancer Dataset. Note that the function's very first argument corresponds to the cos2 results themselves (some previous functions used the PCA objects instead) - let's evaluate the variables first:

#### {.tabset}

##### prcomp()
``` {r corrplot_cos2_1_1}
corrplot(all_pca_var_1$cos2, is.corr=FALSE)
```

##### princomp()
``` {r corrplot_cos2_1_2}
corrplot(all_pca_var_2$cos2, is.corr=FALSE)
```

##### PCA()
``` {r corrplot_cos2_1_3}
corrplot(all_pca_var_3$cos2, is.corr=FALSE)
```

##### dudi.pca()
``` {r corrplot_cos2_1_4}
corrplot(as.matrix(all_pca_var_4$cos2), is.corr=FALSE)
```

##### epPCA()
``` {r corrplot_cos2_1_5}
corrplot(all_pca_var_5$cos2, is.corr=FALSE)
```

#### {-}

Once again, all of the resulting plots are identical - as should be. Note the use of the function <code>as.matrix()</code> within the <code>dudi.pca()</code> tab to transform once again the object of class <code>data.frame</code> to one of class <code>matrix</code> so that the function <code>corrplot()</code> can use it as an argument without issues.<br>

Applying <code>corrplot()</code> upon the individuals' cos2 would yield a massive plot due to the sheer amount of individuals. Given that, the following code snippets are not rendered in order to keep the document clean and readable.

#### {.tabset}

##### prcomp()
``` {r corrplot_cos2_1_6, eval = FALSE}
corrplot(all_pca_ind_1$cos2, is.corr=FALSE)
```

##### princomp()
``` {r corrplot_cos2_1_7, eval = FALSE}
corrplot(all_pca_ind_2$cos2, is.corr=FALSE)
```

##### PCA()
``` {r corrplot_cos2_1_8, eval = FALSE}
corrplot(all_pca_ind_3$cos2, is.corr=FALSE)
```

##### dudi.pca()
``` {r corrplot_cos2_1_9, eval = FALSE}
corrplot(as.matrix(all_pca_ind_4$cos2), is.corr=FALSE)
```

##### epPCA()
``` {r corrplot_cos2_1_10, eval = FALSE}
corrplot(all_pca_ind_5$cos2, is.corr=FALSE)
```

#### {-}

### 2.2.10. Correlation Circle
As was stated previously, there is no well-accepted objective way to decide how many Principal Components are enough - this will depend on the specific field of application and the specific dataset (biomedical scenarios tend to require high cummulative variance since the people's health is at play). However, the first few principal components are the most important ones in order to find interesting patterns in the data and undoubtedly the most important ones when it comes to representing the data.<br>

The correlation circle showcases the correlation between the original dataset features/variables and the Principal Components via coordinates within a 2D circle: the dimension with the most explained variance is the first Principal Component (PC1) and is plotted on the horizontal axis, whereas the second most explanatory dimension is the second Principal Component (PC2) and placed on the vertical axis; the original features/variables are then projected upon this bi-dimensional factor space.<br>

<span class = "quote">"The observations are represented by their projections, but the variables are represented by their correlations."</span><br>
<span class = "quoted">Abdi and Williams, 2010</span>

The correlation circle allows to easily visualize said correlations: if two given lines are pointing in the same direction that implies their associated features/variables are highly correlated, if they are orthogonal they are mostly unrelated and if they are pointing in opposite directions they are negatively correlated.<br>

Plotting correlation circles within R requires the use of the <code>fviz_pca_var()</code> function from the <strong>factoextra</strong> package.

``` {r fviz_pca_var_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(factoextra)
fviz_pca_var(X,
             axes = c(1, 2),
             geom = c("arrow", "text"),
             geom.var = geom,
             repel = FALSE,
             col.var = "black",
             fill.var = "white",
             alpha.var = 1,
             col.quanti.sup = "blue",
             col.circle = "grey70",
             select.var = list(name = NULL, cos2 = NULL, contrib = NULL),
             gradiant.cols = NULL,
             ...)
```

Let's detail its arguments:
<ul>
  <li><strong>X</strong>: a PCA object.</li>
  <li><strong>axes</strong>: a numeric vector of length 2 specifying the dimensions to be plotted.</li>
  <li><strong>geom</strong>: a text specifying the geometry to be used for the graph - allowed values are the combination of <code>c("point", "arrow", "text")</code>.</li>
  <li><strong>geom.var</strong>: as <code>geom</code> but for variables.</li>
  <li><strong>repel</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to use ggrepel to avoid overplotting text labels or not.</li>
  <li><strong>col.var</strong>: a color for variables.</li>
  <li><strong>fill.var</strong>: a fill color for variables.</li>
  <li><strong>alpha.var</strong>: controls the transparency of the variables' colors.</li>
  <li><strong>col.quanti.sup</strong>: a color for the quantitative supplementary variables.</li>
  <li><strong>col.circle</strong>: a color for the correlation circle.</li>
  <li><strong>select.var</strong>: a selection of variables to be drawn.</li>
  <li><strong>gradient.cols</strong>: vector of colors to use for n-colour gradient. Allowed values include brewer and ggsci color palettes.</li>
</ul>

More information regarding the <code>fviz_pca_var()</code> function and all of its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_pca <br>

Let's observe the results of applying the function at hand to the previously constructed PCA objects:

#### {.tabset}

##### prcomp()
``` {r fviz_pca_var_1_1}
fviz_pca_var(all_pca_1, 
             repel = TRUE
             )
```

##### princomp()
``` {r fviz_pca_var_1_2}
fviz_pca_var(all_pca_2, 
             repel = TRUE
             )
```

##### PCA()
``` {r fviz_pca_var_1_3}
fviz_pca_var(all_pca_3, 
             repel = TRUE
             )
```

##### dudi.pca()
``` {r fviz_pca_var_1_4}
fviz_pca_var(all_pca_4, 
             repel = TRUE
             )
```

##### epPCA()
``` {r fviz_pca_var_1_5}
fviz_pca_var(all_pca_5, 
             repel = TRUE
             )
```

#### {-}

Note that, despite them being rotated, the resulting plots are identical - as should be.<br>

The function's arguments allow the colors of the correlation circle to be based upon results for variables obtained through the <code>get_pca_var()</code> function, such as the contribution and quality of representation - the following code snippets showcase said examples (using <code>fviz_pca_var()</code> upon <code>PCA()</code>'s resulting object).

#### {.tabset}

##### Contribution-based
``` {r fviz_pca_var_1_6}
fviz_pca_var(all_pca_3, 
             col.var = "contrib",
             repel = TRUE,
             gradient.cols = c("#FF0000", "#00FF00", "#0000FF")
             # The higher the contrib values, the closer to the last color (blue)
             )
```

##### Cos2-based
``` {r fviz_pca_var_1_7}
fviz_pca_var(all_pca_3, 
             col.var = "cos2",
             repel = TRUE,
             gradient.cols = c("#FF0000", "#00FF00", "#0000FF")
             # The higher the cos2 values, the closer to the last color (blue)
             )
```

#### {-}

Its also possible to change the color of variables by groups defined by a qualitative/categorical variable, commonly known as a factor (and thus, factor-based coloring) - in a correlation circle, this can help to illustrate which groups of variables are highly correlated and which are not.<br>

There are multiple approaches as to how to create appropriate clusters; this document and the following code snippets showcase the <strong>kmeans</strong> clustering algorithm, which aims to partition the points (in this case, the variables) into "k" groups (hence the name) so that the sum of squares from the points to the assigned cluster centers is minimized - the function to perform this clustering algorithm is called <code>kmeans()</code> and is a built-in R function (from R's built-in <strong>stats</strong> package, like the <code>prcomp()</code> and <code>princomp</code> functions previously detailed).

``` {r kmeans_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
kmeans(x, 
       centers, 
       iter.max = 10, 
       nstart = 1,
       algorithm = c("Hartigan-Wong", "Lloyd", "Forgy", "MacQueen"), 
       trace=FALSE)
```

Let's detail the function's arguments:
<ul>
  <li><strong>x</strong>: a numeric matrix of data.</li>
  <li><strong>centers</strong>: "k" - the number of clusters (or a set of cluster centers).</li>
  <li><strong>iter.max</strong>: the maximum number of iterations allowed.</li>
  <li><strong>nstart</strong>: if <code>centers</code> is a number (meaning that is not a set of cluster centers), then this argument determines the amount of random sets chosen.</li>
  <li><strong>algorithm</strong>: a character to determine the underlying algorithm of the k-means clustering - must be one of <code>"Hartigan-Wong"</code>, <code>"Lloyd"</code>, <code>"Forgy"</code> or <code>"MacQueen"</code>.</li>
  <li><strong>trace</strong> (only used in the default method, <code>"Hartigan-Wong"</code>): either a logical or an integer number - if positive (or true), tracing information on the progress of the algorithm is produced. Higher values may produce more tracing information.</li>
</ul>

More information about this function, its behavior and its arguments can be found in its associated RDocumentation page: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kmeans <br>

The following code snippet showcases this function's use in order to create "k" different clusters (the different tabs are meant to illustrate different values of "k"), which are then latter used within <code>fviz_pca_var()</code> to create a correlation circle with factor-based coloring. Note that main argument of <code>kmeans()</code> uses the <strong>coordinates</strong> from the results for variables previously obtained through <code>get_pca_var()</code> (also note that the object and variables at play are the ones obtained with the <code>PCA()</code> function).

#### {.tabset}

##### k = 6
``` {r fviz_pca_var_cluster_1}
# Cluster Creation
res.all <- kmeans(all_pca_var_3$coord, centers = 6, nstart = 25)
grp <- as.factor(res.all$cluster)

# Correlation Circle
fviz_pca_var(all_pca_3, 
             col.var = grp,
             repel = TRUE,
             palette = "jco",
             legend.title = "Clusters")
```

##### k = 3
``` {r fviz_pca_var_cluster_2}
# Cluster Creation
res.all <- kmeans(all_pca_var_3$coord, centers = 3, nstart = 25)
grp <- as.factor(res.all$cluster)

# Correlation Circle
fviz_pca_var(all_pca_3, 
             col.var = grp,
             repel = TRUE,
             palette = "jco",
             legend.title = "Clusters")
```

##### k = 2
``` {r fviz_pca_var_cluster_3}
# Cluster Creation
res.all <- kmeans(all_pca_var_3$coord, centers = 2, nstart = 25)
grp <- as.factor(res.all$cluster)

# Correlation Circle
fviz_pca_var(all_pca_3, 
             col.var = grp,
             repel = TRUE,
             palette = "jco",
             legend.title = "Clusters")
```

#### {-}

### 2.2.11. Plot of individuals
If the correlation circle showcases the correlation between the original dataset features/variables and the Principal Components through a projection upon a bi-dimensional factor space defined by the two most relevant PCs, the plot of individuals delivers the same approach with the individuals themselves. As such, this plot of individuals is also useful to easily visualize the correlations among them based on their position within the graph: proximity means that the individuals are highly correlated whereas distanced individuals are mostly unrelated (data-wise).<br>

Plotting correlation circles within R requires the use of the <code>fviz_pca_ind()</code> function from the <strong>factoextra</strong> package.

``` {r fviz_pca_ind_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(factoextra)
fviz_pca_ind(X,
             axes = c(1, 2),
             geom = c("point", "text"),
             geom.ind = geom,
             repel = FALSE,
             habillage = "none",
             palette = NULL,
             addEllipses = FALSE,
             col.ind = "black",
             fill.ind = "white",
             col.ind.sup = "blue",
             alpha.ind = 1,
             select.ind = list(name = NULL, cos2 = NULL, contrib = NULL),
             ...)
```

Let's detail its arguments:
<ul>
  <li><strong>X</strong>: a PCA object.</li>
  <li><strong>axes</strong>: a numeric vector of length 2 specifying the dimensions to be plotted.</li>
  <li><strong>geom</strong>: a text specifying the geometry to be used for the graph - allowed values are the combination of <code>c("point", "arrow", "text")</code>.</li>
  <li><strong>geom.ind</strong>: as <code>geom</code> but for the individuals.</li>
  <li><strong>repel</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to use ggrepel to avoid overplotting text labels or not.</li>
  <li><strong>habillage</strong>: an optional factor variable for coloring the observations by groups.</li>
  <li><strong>palette</strong>: the color palette to be used for coloring or filling by groups.</li>
  <li><strong>addEllipses</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to draw ellipses around the individuals or not (only when <code>habillage</code> is not <code>none</code></li>
  <li><strong>col.ind</strong>: a color for individuals.</li>
  <li><strong>fill.ind</strong>: a fill color for individuals.</li>
  <li><strong>col.ind.sup</strong>: color for supplementary individuals.</li>
  <li><strong>alpha.ind</strong>: controls the transparency of individuals' colors.</li>
  <li><strong>select.ind</strong>: a selection of individuals to be drawn.</li>
  <li><strong>gradient.cols</strong>: vector of colors to use for n-colour gradient. Allowed values include brewer and ggsci color palettes.</li>
</ul>

Note how the arguments are almost identical to those of the <code>fviz_pca_var()</code> function, albeit not exactly equal. More information regarding the <code>fviz_pca_ind()</code> function and all of its arguments is available in its associated RDocumentation page (which is the same as <code>fviz_pca_var()</code>'s, holding both functions' information): https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_pca <br>

Let's observe the results of applying the function at hand to the previously constructed PCA objects:

#### {.tabset}

##### prcomp()
``` {r fviz_pca_ind_1_1}
fviz_pca_ind(all_pca_1, 
             repel = TRUE
             )
```

##### princomp()
``` {r fviz_pca_ind_1_2}
fviz_pca_ind(all_pca_2, 
             repel = TRUE
             )
```

##### PCA()
``` {r fviz_pca_ind_1_3}
fviz_pca_ind(all_pca_3, 
             repel = TRUE
             )
```

##### dudi.pca()
``` {r fviz_pca_ind_1_4}
fviz_pca_ind(all_pca_4, 
             repel = TRUE
             )
```

##### epPCA()
``` {r fviz_pca_ind_1_5}
fviz_pca_ind(all_pca_5, 
             repel = TRUE
             )
```

#### {-}

Once again, the resulting plots are identical - as should be.<br>

The function's arguments allow the colors of the plot of individuals to be based upon the results for individuals obtained through the <code>get_pca_ind()</code> function, such as the contribution and quality of representation - the following code snippets showcase said examples (using <code>fviz_pca_ind()</code> upon <code>PCA()</code>'s resulting object).

#### {.tabset}

##### Contribution-based
``` {r fviz_pca_ind_1_6}
fviz_pca_ind(all_pca_3, 
             col.ind = "contrib",
             repel = TRUE,
             gradient.cols = c("#FF0000", "#00FF00", "#0000FF"),
             # The higher the contrib values, the closer to the last color (blue)
             label = "none" # hide individual labels - no sensible information
             )
```

##### Cos2-based
``` {r fviz_pca_ind_1_7}
fviz_pca_ind(all_pca_3, 
             col.ind = "cos2",
             repel = TRUE,
             gradient.cols = c("#FF0000", "#00FF00", "#0000FF"),
             # The higher the cos2 values, the closer to the last color (blue)
             label = "none" # hide individual labels - no sensible information
             )
```

#### {-}

Its also possible to change the color of variables by groups defined by a qualitative/categorical variable, commonly known as a factor (and thus, factor-based coloring) - in a plot of individuals, this can help to illustrate which individuals are highly correlated and which are not.<br>

As was the case with the variables, the clusters that define the factors are determined through the use of the <code>kmeans()</code> function. The following code snippet showcases this function's use in order to create "k" different clusters (the different tabs are meant to illustrate different values of "k"), which are then latter used within <code>fviz_pca_ind()</code> to create a plot of individuals with factor-based coloring. Note that main argument of <code>kmeans()</code> uses the <strong>coordinates</strong> from the results for individuals previously obtained through <code>get_pca_ind()</code> (also note that the object and individuals at play are the ones obtained with the <code>PCA()</code> function).

#### {.tabset}

##### k = 6
``` {r fviz_pca_ind_cluster_1}
# Cluster Creation
res.all <- kmeans(all_pca_ind_3$coord, centers = 6, nstart = 25)
grp <- as.factor(res.all$cluster)

# Correlation Circle
fviz_pca_ind(all_pca_3, 
             col.ind = grp,
             repel = TRUE,
             palette = "jco",
             legend.title = "Clusters",
             label = "none" # hide individual labels - no sensible information
             )
```

##### k = 3
``` {r fviz_pca_ind_cluster_2}
# Cluster Creation
res.all <- kmeans(all_pca_ind_3$coord, centers = 3, nstart = 25)
grp <- as.factor(res.all$cluster)

# Correlation Circle
fviz_pca_ind(all_pca_3, 
             col.ind = grp,
             repel = TRUE,
             palette = "jco",
             legend.title = "Clusters",
             label = "none" # hide individual labels - no sensible information
             )
```

##### k = 2
``` {r fviz_pca_ind_cluster_3}
# Cluster Creation
res.all <- kmeans(all_pca_ind_3$coord, centers = 2, nstart = 25)
grp <- as.factor(res.all$cluster)

# Correlation Circle
fviz_pca_ind(all_pca_3, 
             col.ind = grp,
             repel = TRUE,
             palette = "jco",
             legend.title = "Clusters",
             label = "none" # hide individual labels - no sensible information
             )
```

#### {-}

Adding ellipses helps to visualize the clusters - the argument <code>addEllipses</code> controls that with a boolean, as previously stated and as shown in the following code snippets.

#### {.tabset}

##### k = 6
``` {r fviz_pca_ind_cluster_4}
# Cluster Creation
res.all <- kmeans(all_pca_ind_3$coord, centers = 6, nstart = 25)
grp <- as.factor(res.all$cluster)

# Correlation Circle
fviz_pca_ind(all_pca_3, 
             col.ind = grp,
             repel = TRUE,
             palette = "jco",
             addEllipses = TRUE,
             legend.title = "Clusters",
             label = "none" # hide individual labels - no sensible information
             )
```

##### k = 3
``` {r fviz_pca_ind_cluster_5}
# Cluster Creation
res.all <- kmeans(all_pca_ind_3$coord, centers = 3, nstart = 25)
grp <- as.factor(res.all$cluster)

# Correlation Circle
fviz_pca_ind(all_pca_3, 
             col.ind = grp,
             repel = TRUE,
             palette = "jco",
             addEllipses = TRUE,
             legend.title = "Clusters",
             label = "none" # hide individual labels - no sensible information
             )
```

##### k = 2
``` {r fviz_pca_ind_cluster_6}
# Cluster Creation
res.all <- kmeans(all_pca_ind_3$coord, centers = 2, nstart = 25)
grp <- as.factor(res.all$cluster)

# Correlation Circle
fviz_pca_ind(all_pca_3, 
             col.ind = grp,
             repel = TRUE,
             palette = "jco",
             addEllipses = TRUE,
             legend.title = "Clusters",
             label = "none" # hide individual labels - no sensible information
             )
```

#### {-}

### 2.2.12. Biplot
As the correlation circle and the plot of individuals, the biplot is a graphing method which approximates the multi-dimensional dataset (along with its data points) by a bi-dimensional matrix defined by the two most relevant Principal Components; in fact, a biplot is kind of combination of both those plots within a single graph and, as such, its plot is dependent on functions from the <strong>factoextra</strong> package: <code>fviz_pca_biplot()</code> and <code>fviz_pca()</code>, whose behavior is identical (one is but an alias of the other).

``` {r fviz_pca_biplot_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(factoextra)
fviz_pca(X, ...)
fviz_pca_biplot(X,
                axes = c(1, 2),
                geom = c("point", "text"),
                geom.ind = geom,
                geom.var = c("arrow", "text"),
                col.ind = "black",
                fill.ind = "white",
                col.var = "steelblue",
                fill.var = "white",
                gradient.cols = NULL,
                label = "all",
                invisible = "none",
                repel = FALSE,
                habillage = "none",
                palette = NULL,
                addEllipses = FALSE,
                title = "PCA - Biplot",
                ...)
```

It can be observed that its arguments are a combination of those available for the <code>fviz_pca_var()</code> and <code>fviz_pca_ind()</code> - there are arguments to tweak either, which makes sense given that the biplot itself is a combination of the correlation circle (for variables) and the plot of individuals. As such, it is also possible to create a biplot based upon the results obtained through <code>get_pca_var()</code> and <code>get_pca_ind()</code>. The following code snippets showcase that: the first tab is the most basic biplot (a colorless one) whereas the second and third tab illustrate a biplot based upon contributions and quality of representation (cos2) respectively (using <code>fviz_pca_biplot()</code> upon <code>PCA()</code>'s resulting object).

#### {.tabset}

##### Colorless
``` {r fviz_pca_biplot_1_1}
fviz_pca_biplot(all_pca_3, 
                col.ind = wbcd$diagnosis, 
                col="black",
                palette = "jco",
                geom = "point",
                repel=TRUE,
                legend.title="Diagnosis", 
                addEllipses = TRUE)
```

##### Contribution-based
``` {r fviz_pca_biplot_1_2}
fviz_pca_biplot(all_pca_3, 
                col.ind = wbcd$diagnosis, 
                col="black",
                palette = "jco",
                geom = "point",
                repel=TRUE,
                legend.title="Diagnosis", 
                addEllipses = TRUE)
```

##### Cos2-based
``` {r fviz_pca_biplot_1_3}
fviz_pca_biplot(all_pca_3, 
                col.ind = wbcd$diagnosis, 
                col="black",
                palette = "jco",
                geom = "point",
                repel=TRUE,
                legend.title="Diagnosis", 
                addEllipses = TRUE)
```

#### {-}

It is worth noting that the RDocumentation page documenting both <code>fviz_pca_var()</code> and <code>fviz_pca_ind()</code> also details the biplot functions at hand, so any additional information regarding these functions, their behavior and their arguments is available at https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_pca <br>

Let's observe the results of applying the function <code>fviz_pca_biplot()</code> to the previously constructed PCA objects.

# 3. Machine Learning
Machine learning is a branch/subset of artificial intelligence (AI) and an important component of the growing field of data science which mimics the way humans learn by using certain algorithms that improve their accuracy upon training, a process that loops through the sampled data contrasting guesses with real values to evaluate the algorithm's accuracy so that it can develop a statistical model which maximizes said accuracy and best fits the supplied data. These models can be used in <strong>classification</strong> and <strong>regression</strong>-based scenarios (to predict integers/factors and continuous values, respectively) as they uncover key insights and relationships from within the data that are hidden to the human eye and could take years to take grasp of.<br>

This chapter aims to apply a variety of machine learning approaches to the dataset at hand so that the algorithms can determine whether any given patient's cancer is benign or malign. The goals of this classification exercise include the exploration of the machine learning approaches, its application within R and a comparison of their accuracy in order to determine/choose the one that best fits within this scenario.<br>

## 3.1. Machine Learning Algorithms
Machine learning algorithms are often categorized as either <strong>supervised learning</strong> or <strong>unsupervised learning</strong>. The former, as the name suggests, requires a supervisor/user that feeds the algorithm with well labeled data so that the algorithm can learn/train whereas the latter (unsupervised) is being fed information that is neither classified nor labeled allowing the algorithm to act without guidance, grouping such information in clusters according to similarities, patters and differences found in the data. Note that due to the nature of this clustering process, unsupervised machine learning approaches are rarely seen outside of classification scenarios, but since this exercise is of such kind it works well with both supervised and unsupervised algorithms.<br>

The very first step is to divide the dataset at hand in two subsets: a training one, which will be used to train/educate the algorithm, and a testing one, which will be used to evaluate the algorithm's effectiveness. This subdivision can be achieved using the R built-in functions <code>nrow()</code> and <code>sample()</code> to randomly select row indexes from within the dataset and thus create both the training and the testing sets pseudo-randomly.

``` {r train_test_1_1}
train_index <- sample(1:nrow(wbcd), 0.7*nrow(wbcd))
train_set <- wbcd[train_index,]
test_set <- wbcd[-train_index,]

dim(wbcd) # 569 test data
dim(train_set) # 398/569 test data (70%)
dim(test_set) # 171/569 test data (30%)
```

The previous code snippet showcases the construction of the training and testing sets, although built-in R functions are not the usual/preferred approach. The <strong>caret</strong> package (short for Classification And Regression Training) contains functions to streamline many machine learning tasks and, as such, it is undoubtedly one of the most popular libraries for the matter. Among its many functions lies <code>createDataPartition()</code>, which divides the working dataset into the training and testing subsets while keeping the classification ratio constant within each set - that means that if the dataset is to be divided into a training set that holds 70% of the data and a testing set that holds the missing 30%, then each will have the same factor distribution (from which the algorithm will learn to classify) as the original dataset, avoiding certain unfavorable scenarios where there might not be a sufficient amount of a given factor within the training set to allow the algorithm to develop a fitting model.

``` {r createDataPartition, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(caret)
createDataPartition(
  y,
  times = 1,
  p = 0.5,
  list = TRUE,
  groups = min(5, length(y))
)
```

The arguments of the function are as follows:
<ul>
  <li><strong>y</strong>: a vector of outcomes.</li>
  <li><strong>times</strong>: the number of partitions to create.</li>
  <li><strong>p</strong>: the percentage of data that goes to training.</li>
  <li><strong>list</strong>: whether to hold the results within a list or within a matrix.</li>
  <li><strong>groups</strong>: if <code>y</code> (the vector of outcomes) is numerical, then this argument defines the number of breaks in the quantiles.</li>
</ul>

Note that, as opposed to the previous approach, the main argument of the function does not ask for the dataset and uses a vector of outcomes instead (in the case of this exercise said vector of outcomes is the diagnosis array). More information about the function, its behavior and its arguments can be found in its associated RDocumentation page: https://www.rdocumentation.org/packages/caret/versions/6.0-90/topics/createDataPartition <br>

The following code snippet showcases the subset construction using the <code>createDataPartition()</code> function. Note that the balance is 70% for the training set and 30% for the testing one, which is a commonly used ratio for the train-test split (an in fact such was the ratio previously used with the R built-in functions).

``` {r train_test_1_2}
library(caret)
train_index <- createDataPartition(wbcd$diagnosis, times = 1, p = 0.7, list = FALSE)
train_set <- wbcd[train_index,]
test_set <- wbcd[-train_index,]

dim(wbcd) # 569 test data
dim(train_set) # 398/569 test data (70%)
dim(test_set) # 171/569 test data (30%)
```

Once the training and testing sets are constructed, the next step is to apply the machine learning algorithm of choice. Many of these algorithms are covered within this chapter, and every single one of them is worthy enough of a document of its own detailing the intricacies of their behavior and inner working - such task goes beyond the scope of this project although there will be an overview briefly describing each of these machine learning approaches.

### 3.1.1. Linear Regression

Linear regression can be considered a machine learning algorithm despite its simplicity and rigidness (which makes it unreliable in most cases). It works rather well under certain scenarios and can be used to obtain a quick reference given the algorithm's speed (processing-wise it is less demanding than most other machine learning approaches).<br>

The R built-in <strong>stats</strong> package already bundles a set of functions to build linear regression models, namely <code>lm()</code> and <code>glm()</code>. Let's detail the former, which is the one related to linear regression (the latter corresponds to logistic regression, which will be detailed later on):

``` {r lm_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(stats)
lm(formula, 
   data,
   subset,
   weights,
   na.action,
   method = "qr",
   model = TRUE,
   x = FALSE,
   y = FALSE,
   qr = TRUE,
   singular.ok = TRUE,
   contrasts = NULL,
   offset,
   ...
)
```

Let's detail its arguments:
<ul>
  <li><strong>formula</strong>: an object of class <code>formula</code> (or one that can be coarced to that class).</li>
  <li><strong>data</strong>: the data frame containing the variables in the model.</li>
  <li><strong>subset</strong>: an optional vector specifying a subset of observations to be used in the fitting process.</li>
  <li><strong>weights</strong>: an optional vector of weights to be used in the fitting process.</li>
  <li><strong>na.action</strong>: a function which indicates what should happen when the data contain <code class = "boolean">NA</code>.</li>
  <li><strong>method</strong>: the method to be used for fitting, although at the time of writing only <code>method = <span class = "string">"qr"</span></code></li>
  <li><strong>model, x, y, qr</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to return the corresponding components of the fit (the model frame, the model matrix, the response, the QR decomposition respectively).</li>
  <li><strong>singular.ok</strong>: if <code class = "boolean">FALSE</code> a singular fit yields an error.</li>
  <li><strong>contrasts</strong>: an optional list of contrasts to use with the model matrix.</li>
  <li><strong>offset</strong>: this can be used to specify an a priori known component to be included in the linear predictor during fitting.</li>
</ul>

More information regarding the <code>lm()</code> function and all of its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm <br>

After feeding the function with the proper information, a model is built:

``` {r lm_1_1}
library(stats)
model_linear <- lm(diagnosis ~ ., data=train_set)
```

Once the model is built, the function <code>predict()</code> is used to apply the constructed model upon the testing subset's features in order to classify the set's items. Note that the predicted data needs to be properly factored, for which one can use the <code>factor()</code> function along with a cutoff value (in this case the mean predicted value is used) to tag the guesses.<br>

The function <code>confusionMatrix()</code> from the <strong>caret</strong> package can be used to visualize the accuracy/success of the algorithm. Confusion matrices are widely used in the data science and machine learning fields since they showcase said accuracy/success through various metrics in an easy-to-understand table. The following code snippet illustrates the use of these functions (<code>predict()</code> and <code>confusionMatrix()</code>), both of which are core functions within the data science and machine learning fields.

``` {r lm_1_2}
library(caret)
prediction_linear <- predict(model_linear, test_set)
prediction_linear <- factor(ifelse(prediction_linear > mean(prediction_linear), "Malignant", "Benign"))
cm_linear <- confusionMatrix(prediction_linear, test_set$diagnosis)
cm_linear
```

There are numerous metrics to measure a machine learning algorithm accuracy/success, but it is practical/useful to have a one number summary. The overall accuracy is somewhat misleading since it does not account for the sensitivity and specificity of the model, so alternatives such as the balanced accuracy (which is the average of specificity and sensitivity) or the F1 score (the harmonic average of precision and recall) are preferred.  
Accessing these values from the confusion matrix can be achieved using <code>$byClass</code> as is showcased in the following code snippet:

``` {r lm_1_3}
acc_linear <- cm_linear$byClass['Balanced Accuracy']
F1_linear <- cm_linear$byClass['F1']
print(c(acc_linear, F1_linear))
```

<!-- Given both metrics' importance, the one value to use in order to measure and compare the results and overall success of this algorithm to those of alternative approaches is the average of them both.

``` {r lm_1_4}
success_linear <- mean(acc_linear, F1_linear)
```

-->
### 3.1.2. Logistic Regression

A slightly more advanced version of the linear model just described is found within logistic regression. Its fit follows a logistic curve pattern allowing it to better represent non-linear data distributions.<br>

As was the case with the <code>lm()</code> function, the function <code>glm()</code> comes from the built-in <strong>stats</strong> package, meaning that no additional libraries need to be imported.

``` {r glm_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(stats)
glm(formula,
    data,
    family = gaussian,
    weights,
    subset,
    na.action,
    start = NULL,
    etastart,
    mustart,
    offset,
    control = list(...),
    method = "glm.fit",
    model = TRUE,
    x = FALSE,
    y = TRUE,
    singular.ok = TRUE,
    contrasts = NULL,
    ...
)
```

Let's detail its arguments:
<ul>
  <li><strong>formula</strong>: an object of class <code>formula</code> (or one that can be coarced to that class).</li>
  <li><strong>data</strong>: the data frame containing the variables in the model.</li>
  <li><strong>family</strong>: a description of the error distribution and link function to be used in the model (e.g. a character string naming a family function, a family function or the result of a call to a family function).</li>
  <li><strong>weights</strong>: an optional vector of weights to be used in the fitting process.</li>
  <li><strong>subset</strong>: an optional vector specifying a subset of observations to be used in the fitting process.</li>
  <li><strong>na.action</strong>: a function which indicates what should happen when the data contain <code class = "boolean">NA</code> data.</li>
  <li><strong>start</strong>: starting values for the parameters in the linear predictor.</li>
  <li><strong>etastart</strong>: starting values for the linear predictor.</li>
  <li><strong>mustart</strong>: starting values for the vector of means.</li>
  <li><strong>offset</strong>: this can be used to specify an a priori known component to be included in the linear predictor during fitting.</li>
  <li><strong>control</strong>: a list of parameters for controlling the fitting process.</li>
  <li><strong>method</strong>: the method to be used for fitting. The default method <code class = "string">"glm.fit"</code> uses iteratively reweighted least squares (IWLS) whereas the alternative <code class = "string">"model.frame"</code> returns the model frame and does no fitting.</li>
  <li><strong>model</strong>: a logical value indicating whether <i>model frame</i> should be included as a component of the returned value.</li>
  <li><strong>x, y</strong>: logical values indicating whether the response vector and model matrix used in the fitting process should be returned as components of the returned value.</li>
  <li><strong>singular.ok</strong>: if <code class = "boolean">FALSE</code> a singular fit yields an error.</li>
  <li><strong>contrasts</strong>: an optional list of contrasts to use with the model matrix.</li>
</ul>

More information regarding the <code>lm()</code> function and all of its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm <br>

The following code snippet showcases the construction of the model, its associated prediction and confusion matrix and the most relevant one number summary metrics:

``` {r glm_1_1}
library(stats)
library(caret)

model_logistic <- glm(diagnosis ~ ., data=train_set, family = "binomial")
prediction_logistic <- predict(model_logistic, test_set)
prediction_logistic <- factor(ifelse(prediction_logistic > mean(prediction_logistic), "Malignant", "Benign"))
cm_logistic <- confusionMatrix(prediction_logistic, test_set$diagnosis)
cm_logistic

acc_logistic <- cm_logistic$byClass['Balanced Accuracy']
F1_logistic <- cm_logistic$byClass['F1']
print(c(acc_logistic, F1_logistic))
```

### 3.1.3. C5.0

The <strong>C50</strong> package contains an interface to the C5.0 classification model. More information regarding this package, its functions and their inner working can be found in https://cran.r-project.org/web/packages/C50/vignettes/C5.0.html <br>

The most important note about the package is that it uses decision trees as its core. Decision trees can be used to visually and explicitly represent decisions and decision making through, as the name implies, a tree-like model. Visually speaking, these trees are drawn upside down with its root at the top, branching through conditionals through a downwards reading - the end of a branch is known as the "leaf" and represents the algorithm's decision.  
Decision trees are used in machine learning covering both classification and regression scenarios: classification trees predicts the class/factor of an item given a set of features whereas regression trees behave in the same manner although predicting continuous values instead. The <strong>C50</strong> package is built around the <code>C5.0()</code> function, which fits a classification tree model upon the dataset in order to train the algorithm so that the tree model learns which features to choose and what conditions to use for splitting/branching (by constantly looping through the constructed tree and comparing the obtained hypothetical results with the real ones provided by the training set).<br>

``` {r C50_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(C50)
C5.0(
  x,
  y,
  trials = 1,
  rules = FALSE,
  weights = NULL,
  control = C5.0Control(),
  costs = NULL,
  ...
)
```

Let's detail its arguments:
<ul>
  <li><strong>x</strong>: the predictors (the data features required for the algorithm so that it can develop the decision tree).</li>
  <li><strong>y</strong>: a factor vector with 2 or more levels; note that the <code>C5.0()</code> function fits a classification tree, and this argument clearly shows that the function can not fit a regression fit since it only accepts factors as input.</li>
  <li><strong>trials</strong>: an integer specifying the number of boosting iterations. A value of one indicates that a single model is used.</li>
  <li><strong>rules</strong>: a logical determining whether or not the tree should be decomposed into a rule-based model.</li>
  <li><strong>weights</strong>: an optional numeric vector of case weights.</li>
  <li><strong>control</strong>: a list of control parameters (see https://www.rdocumentation.org/packages/C50/versions/0.1.5/topics/C5.0Control)</li>
  <li><strong>costs</strong>: a matrix of costs associated with the possible errors.</li>
</ul>

More information regarding the <code>C5.0()</code> function and all of its arguments is available in its associated RDocumentation page: https://www.rdocumentation.org/packages/C50/versions/0.1.5/topics/C5.0.default <br>

The following code snippet showcases the construction of the model, its associated prediction and confusion matrix and the most relevant one number summary metrics:

``` {r C50_1_1}
library(C50)
library(caret)

model_C50 <- C5.0(train_set[,-1], train_set$diagnosis)
prediction_C50 <- predict(model_C50, test_set[,-1])
cm_C50 <- confusionMatrix(prediction_C50, test_set$diagnosis)
cm_C50

acc_C50 <- cm_C50$byClass['Balanced Accuracy']
F1_C50 <- cm_C50$byClass['F1']
print(c(acc_C50, F1_C50))
```

Note that the prediction metrics can be improved by changing the arguments constructing the function, namely the <code>trials</code> input - the following code snippet showcases a loop within which a table is created to compare the accuracy results obtained with various <code>trials</code> values (in between 1 and 50). Note that this process is usually known as <strong>tuning</strong>, and consists in tweaking the model so that it fits/works better with the data it is being fed so that a higher accuracy can be achieved.

``` {r C50_1_2}
acc_C50_array <- NULL
F1_C50_array <- NULL

for(i in 1:50){
    model_C50_temp <- C5.0(train_set[,-1], train_set$diagnosis, trials = i)      
    prediction_C50_temp <- predict(model_C50_temp, test_set[,-1]) 
    cm_C50_temp <- confusionMatrix(prediction_C50_temp, test_set$diagnosis)
    acc_C50_array[i] <- cm_C50_temp$byClass['Balanced Accuracy']
    F1_C50_array[i] <- cm_C50_temp$byClass['F1']
}

acc_C50_df <- data.frame(trials = seq(1,50), acc = acc_C50_array)
acc_C50_optimal <- subset(acc_C50_df, acc == max(acc))[1,]

F1_C50_df <- data.frame(trials = seq(1,50), F1 = F1_C50_array)
F1_C50_optimal <- subset(F1_C50_df, F1 == max(F1))[1,]

print(c(acc_C50_optimal, F1_C50_optimal)) # At the time of writing these values coincide, but that might not be the case

tuning_C50_df <- data.frame(trials = seq(1,50), success = 0.5 * (acc_C50_array + F1_C50_array)) # We average F1 and balanced accuracy to measure success
library(dplyr) # For the mutate() function used to add balanced accuracy and F1 values to the dataframe
tuning_C50 <- subset(tuning_C50_df, success == max(success))[1,] %>% mutate(acc = acc_C50_df[max(trials), 2], F1 = F1_C50_df[max(trials), 2])
print(tuning_C50)
```

Through tuning, a higher accuracy/success can be achieved. Using a value of `r tuning_C50$trials` for the algorithm trials increases the balanced accuracy from `r acc_C50` to `r tuning_C50$acc` and the F1 score from `r F1_C50` to `r tuning_C50$F1`, which is a considerable improvement upon the previous results. To see how the number of trials affects the accuracy values a graph can be plotted - for illustration purposes, said plot is performed via two different libraries: <strong>highcharter</strong> and <strong>ggplot2</strong>.

#### {.tabset}

##### highcharter

``` {r C50_1_3}
sub_C50 <- paste("Optimal number of trials is", tuning_C50$trials, "with an averaged success (balanced accuracy and F1 score) of ", tuning_C50$success)

library(highcharter)
hchart(tuning_C50_df, 'line', hcaes(trials, success)) %>%
  hc_title(text = "Averaged success with varying trials (C5.0)") %>%
  hc_subtitle(text = sub_C50) %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_xAxis(title = list(text = "Number of trials")) %>%
  hc_yAxis(title = list(text = "Averaged success"))
```

##### ggplot2

``` {r C50_1_4}
sub_C50 <- paste("Optimal number of trials is", tuning_C50$trials, "with an averaged success (balanced accuracy and F1 score) of ", tuning_C50$success)

library(ggplot2)
ggplot(tuning_C50_df, aes(trials, success)) + 
  geom_line() + 
  geom_point() + theme_minimal() + 
  labs(title = "Averaged success with varying trials (C5.0)",
       subtitle = sub_C50,
       x = "Number of trials",
       y = "Averaged success")
```

#### {-}

### 3.1.4. rpart

The <strong>rpart</strong> package allows to easily build classification or regression models with a very general structure (meaning that they can be applied in multiple cases/scenarios) using a two stage procedure which follow a decision tree behavior (already detailed within the C5.0 chapter). More information regarding this package can be found in its associated RDocumentation page: https://www.rdocumentation.org/packages/rpart <br>

The core of the package lies within its main function: <code>rpart()</code>.

``` {r rpart_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(rpart)
rpart(formula, 
      data,
      weights,
      subset,
      na.action = na.rpart,
      method,
      model = FALSE,
      x = FALSE, 
      y = TRUE, 
      parms, 
      control, 
      cost,
      ...)
```

The function's arguments are as follows:
<ul>
  <li><strong>formula</strong>: the model formula (should look like <code>feature ~ predictor</code>).</li>
  <li><strong>data</strong>: an optional data frame in which to interpret the variables named in the formula.</li>
  <li><strong>weights</strong>: optional case weights.</li>
  <li><strong>subset</strong>: optional expression saying that only a subset of the rows of the data should be used in the fit.</li>
  <li><strong>na.action</strong>: the default action deletes all observations for which <code>y</code> is missing, but keeps those in which one or more predictors are missing.</li>
  <li><strong>method</strong>: one of <code class = "string">"anova"</code>, <code class = "string">"poisson"</code>, <code class = "string">"class"</code> or <code class = "string">"exp"</code>. If method is missing then the routine tries to make an intelligent guess, which is one of the strengths of this function/package:
    <ul>
      <li>If <code>y</code> is a survival object, then <code>method = <span class = "string">"exp"</span></code> is assumed.</li>
      <li>If <code>y</code> has 2 columns, then <code>method = <span class = "string">"poisson"</span></code> is assumed.</li>
      <li>If <code>y</code> is a factor, then <code>method = <span class = "string">"class"</span></code> is assumed.</li>
      <li>Otherwise <code>method = <span class = "string">"anova"</span></code> is assumed.</li>
    </ul>
  </li>
  <li><strong>model</strong>: can be a boolean, where <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determine whether to keep a copy of the model frame in the result or not; can also be a model frame, in which case said frame is used rather than constructing new data.</li>
  <li><strong>x</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determine whether to keep a copy of the <code>x</code> matrix in the result or not.</li>
  <li><strong>y</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determine whether to keep a copy of the dependent variable in the result or not. If missing and <code>model</code> is supplied this defaults to <code class = "boolean">FALSE</code>.</li>
  <li><strong>parms</strong>: optional parameters for the splitting function.
    <ul>
      <li>Anova splitting has no parameters.</li>
      <li>Poisson splitting has a single parameter, the coefficient of variation of the prior distribution on the rates (default is 1).</li>
      <li>Exponential splitting has the same parameter as Poisson.</li>
      <li>For classification splitting, the list can contain any of: the vector of prior probabilities (component <code>prior</code>, the loss matrix (component <code>loss</code>) or the splitting index (component <code>split</code>).</li>
        <ul>
          <li>The priors must be positive and sum to 1. Note that the default priors are proportional to the data counts.</li>
          <li>The loss matrix must have zeros on the diagonal and positive off-diagonal elements. Note that the losses default to 1.</li>
          <li>The splitting index can be <code class = "string">gini</code> or <code class = "string">information</code>.</li>
        </ul>
    </ul>
  </li>
  <li><strong>control</strong>: the function accepts additional arguments that can be passed to <code>rpart.control</code>; see https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart.control </li>
  <li><strong>cost</strong>: a vector of non-negative costs, one for each variable in the model (defaults to one for all variables). These are scalings to be applied when considering splits, so the improvement on splitting on a variable is divided by its cost in deciding which split to choose.</li>
</ul>

More information about this function, its behavior and its arguments can be found in its associated RDocumentation page: https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart <br>

Despite the amount of customization this function allows, the following code snippet showcases its use, prediction, confusion matrix and relevant one number summary metrics:

``` {r rpart_1_1}
library(rpart)
library(caret)
model_rpart <- rpart(diagnosis ~ ., data = train_set)
prediction_rpart <- predict(model_rpart, test_set[,-1], type = "class")
cm_rpart <- confusionMatrix(prediction_rpart, test_set$diagnosis)
cm_rpart

acc_rpart <- cm_rpart$byClass['Balanced Accuracy']
F1_rpart <- cm_rpart$byClass['F1']
print(c(acc_rpart, F1_rpart))
```

At the time of writing, the results obtained with this function are somewhat unsatisfactory. Tuning this model could theoretically yield better results, but my tinkering with its arguments has only led to worse accuracy values - [here's an interesting article](https://www.gormanalysis.com/blog/decision-trees-in-r-using-rpart/ "Decision Trees in R using rpart") about the function's behavior that could help anyone tune the function to its needs and data, but this document needs to move on to the following machine learning approach (time is but the most valuable currency).

### 3.1.5. RWeka

Weka is a collection of machine learning algorithm for data mining tasks written in Java, containing tools for data pre-processing, classification, regression, clustering, association rules, and visualization. The package <strong>RWeka</strong> is but an R interface to said collection, bringing the Weka toolset to the R environment.

``` {r RWeka_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library("RWeka")
JRip(formula, 
     data, 
     subset, 
     na.action,
     control = Weka_control(), 
     options = NULL)

M5Rules(formula, 
        data, 
        subset, 
        na.action,
        control = Weka_control(), 
        options = NULL)

OneR(formula, 
     data, 
     subset, 
     na.action,
     control = Weka_control(), 
     options = NULL)

PART(formula, 
     data, 
     subset, 
     na.action,
     control = Weka_control(), 
     options = NULL)
```

The functions' arguments are as follows:
<ul>
  <li><strong>formula</strong>: a symbolic description of the model to be fit.</li>
  <li><strong>data</strong>: an optional data frame containing the variables in the model.</li>
  <li><strong>subset</strong>: an optional vector specifying a subset of observations to be used in the fitting process.</li>
  <li><strong>na.action</strong>:  the default action deletes all observations for which <code>y</code> is missing, but keeps those in which one or more predictors are missing.</li>
  <li><strong>control</strong>: an object of class <code>Weka_control</code> giving options to be passed to the Weka learner (see [the associated documentation](https://www.rdocumentation.org/packages/RWeka/topics/Weka_control "Control Weka Options")).</li>
  <li><strong>options</strong>: a named list of further options, or NULL (default).</li>
</ul>

Note that additional information about these functions, their behavior and their arguments can be found in [their associated RDocumentation page](https://www.rdocumentation.org/packages/RWeka/topics/Weka_classifier_rules "Weka Rule Learners").<br>

The <code>JRip()</code> function is based upon "RIPPER", an acronym standing for "Repeated Incremental Pruning to Produce Error Reduction" which, as the name suggests, prunes a decision tree to avoid overfitting and minimize/reduce (potential) error. Its foundations are detailed in [Cohen's work](https://www.sciencedirect.com/science/article/pii/B9781558603776500232 "Fast Effective Rule Induction: RIPPER") (its author/creator).<br>

The <code>M5Rules()</code> function generates a decision tree using a "separate-and-conquer" approach where each iteration constructs a tree using [a given set of rules](https://www.researchgate.net/profile/Saipoornachand-Kolli/publication/344628947_Assessment_Analysis_and_Performance_Prediction_using_M5_Rules/links/5f85803f299bf1b53e230e22/Assessment-Analysis-and-Performance-Prediction-using-M5-Rules.pdf "Assessment Analysis and Performance
Prediction using M5 Rules") and turns the "best" leaf into a rule. However, <code>M5Rules()</code> is strictly used within regression exercises, so it cannot be applied in classification tasks such as the ones being performed throughout this document.

The <code>OneR()</code> function builds a simple yet effective and useful "one-rule" classifier, also known as Holte's classifier or Holte's 1R classifier after its creator/developer (see [the original paper](https://link.springer.com/article/10.1023/A:1022631118932 "Very Simple Classification Rules Perform Well on Most Commonly Used Datasets") or [Nevill-Manning et al.'s take on it](https://ieeexplore.ieee.org/abstract/document/499480 "The development of Holte's 1R classifier")). While technically based upon "one feature" and not "one rule", fact is that its name comes from the fact that it finds exactly one feature (and one or more feature values for that feature) to classify data instances, which makes it a fast and simple approach although it is worth noting that it is not known for its good prediction performance (it is rather recommended for teaching purposes and for lower-bound performance baselines in real-world applications).<br>

The <code>PART()</code> function is based upon "RIPPER", an acronym standing for "Repeated Incremental Pruning to Produce Error Reduction" which, as the name suggests, prunes a decision tree to avoid overfitting and minimize/reduce (potential) error. Its foundations are detailed in [Cohen's work](https://www.sciencedirect.com/science/article/pii/B9781558603776500232 "Fast Effective Rule Induction: RIPPER") (its author/creator).<br>

The following code snippet showcases the previously detailed functions albeit <code>M5Rules</code>, which is used in regression exercises and does not fit classification exercises such as this one. The showcase illustrates the construction of the models, their associated predictions and confusion matrices as well as the most relevant one number summary metrics (which can be used to compare their predictions' success):

``` {r RWeka_1_1}
library("RWeka")

model_JRip <- JRip(diagnosis~., data = train_set)
predict_JRip <- predict(model_JRip, test_set[,-1])
cm_JRip  <- confusionMatrix(predict_JRip, test_set$diagnosis)   
cm_JRip

model_OneR <- OneR(diagnosis~., data = train_set)
predict_OneR <- predict(model_OneR, test_set[,-1])
cm_OneR  <- confusionMatrix(predict_OneR, test_set$diagnosis)   
cm_OneR

model_PART <- PART(diagnosis~., data = train_set)
predict_PART <- predict(model_PART, test_set[,-1])
cm_PART  <- confusionMatrix(predict_PART, test_set$diagnosis)
cm_PART

RWeka_df <- data.frame("Balanced Accuracy" = c(cm_JRip$byClass['Balanced Accuracy'],
                                               cm_OneR$byClass['Balanced Accuracy'],
                                               cm_PART$byClass['Balanced Accuracy']),
                       "F1" = c(cm_JRip$byClass['F1'],
                                cm_OneR$byClass['F1'],
                                cm_PART$byClass['F1']),
                       row.names = c("JRip", "OneR", "PART"))
RWeka_df
```

As can be seen, `r if(which(RWeka_df['F1'] == max(RWeka_df['F1'])) == which(RWeka_df['Balanced.Accuracy'] == max(RWeka_df['Balanced.Accuracy']))) {paste("<code>", rownames(RWeka_df[which(RWeka_df['F1'] == max(RWeka_df['F1'])),]), "</code> yields the best results out of all of these functions with default settings (perhaps tuning each of these functions could change these results).")} else {paste("there is no clear victor in this comparison since", rownames(RWeka_df[which(RWeka_df['F1'] == max(RWeka_df['F1'])),]), "yields the best F1 score whereas", rownames(RWeka_df[which(RWeka_df['Balanced.Accuracy'] == max(RWeka_df['Balanced.Accuracy'])),]), "yields the best balanced accuracy.")}`

### 3.1.6. Naive Bayes

The <strong>Naive Bayes</strong> algorithm is one of the most popular machine learning approaches. It is based upon the Bayes' theorem, which describes the probability of an event based on prior knowledge of conditions related to said event. The mathematical notation is as follows:
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
Where:
<ul>
  <li>P(A|B): probability of A given B.</li>
  <li>P(B|A)>:  probability of B given A.</li>
  <li>P(A): probability of A.</li>
  <li>P(B): probability of B.</li>
</ul>

The Naive Bayes algorithm, also known as the Multinomial Naive Bayes Classifier, uses this principle to build its decision tree, developing each branch based upon the probability of each branch given prior (training) knowledge. [Josh Starmer's video on it](https://www.youtube.com/watch?v=H3EjCKtlVog "StatQuest: Gaussian Naive Bayes") visually explains and exemplifies this concept ([he has published an even more simplified video](https://www.youtube.com/watch?v=O2L2Uv9pdDA "StatQuest: Naive Bayes") where the naivity aspect of the algorithm is explained, which briefly speaking is due to the algorithm ignoring potential relationships between features).<br>

There are many libraries which include a Naive Bayes' machine learning function but, for the sake of simplicity, this document will focus around one: the <strgng>e1071</strong> package, which holds various miscellaneous statistics-based functions (from Fourier transformations to many clustering approaches and machine learning algorithms), can make use of this algorithm via the <code>naiveBayes()</code> function.

``` {r naiveBayes_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(e1071)
naiveBayes(x, 
           y, 
           laplace = 0,
           ...
)
```

The functions' arguments are as follows:
<ul>
  <li><strong>x</strong>: a numeric matrix, or a data frame of categorical and/or numeric variables.</li>
  <li><strong>y</strong>: class vector.</li>
  <li><strong>laplace</strong>: positive double controlling Laplace smoothing. The default (0) disables Laplace smoothing.</li>
</ul>

More information about this function, its behavior and its arguments can be found in its associated RDocumentation page: https://www.rdocumentation.org/packages/e1071/versions/1.7-9/topics/naiveBayes <br>

The following code snippet showcases the construction of the model, its associated prediction and confusion matrix and the most relevant one number summary metrics:

``` {r naiveBayes_1_1}
library(e1071)
library(caret)

model_naiveBayes <- naiveBayes(train_set[,-1], train_set$diagnosis)
prediction_naiveBayes <- predict(model_naiveBayes, test_set[,-1], type = "class")
cm_naiveBayes <- confusionMatrix(prediction_naiveBayes, test_set$diagnosis)
cm_naiveBayes

acc_naiveBayes <- cm_naiveBayes$byClass['Balanced Accuracy']
F1_naiveBayes <- cm_naiveBayes$byClass['F1']
print(c(acc_naiveBayes, F1_naiveBayes))
```

Laplace smoothing can be tuned to achieve a better prediction.

``` {r naiveBayes_1_2}
library(e1071)
library(caret)

acc_naiveBayes_array <- NULL
F1_naiveBayes_array <- NULL

for(i in 0:50){
    model_naiveBayes_temp <- naiveBayes(train_set[,-1], train_set$diagnosis, laplace = i) 
    prediction_naiveBayes_temp <- predict(model_naiveBayes_temp, test_set[,-1]) 
    cm_naiveBayes_temp <- confusionMatrix(prediction_naiveBayes_temp, test_set$diagnosis)
    acc_naiveBayes_array[i+1] <- cm_naiveBayes_temp$byClass['Balanced Accuracy']
    F1_naiveBayes_array[i+1] <- cm_naiveBayes_temp$byClass['F1']
}

acc_naiveBayes_df <- data.frame(laplace = seq(0,50), acc = acc_naiveBayes_array)
acc_naiveBayes_optimal <- subset(acc_naiveBayes_df, acc == max(acc))[1,]

F1_naiveBayes_df <- data.frame(laplace = seq(0,50), F1 = F1_naiveBayes_array)
F1_naiveBayes_optimal <- subset(F1_naiveBayes_df, F1 == max(F1))[1,]

print(c(acc_naiveBayes_optimal, F1_naiveBayes_optimal))

# We average F1 and balanced accuracy to measure success
tuning_naiveBayes_df <- data.frame(laplace = seq(0,50), success = 0.5 * (acc_naiveBayes_array + F1_naiveBayes_array))
tuning_naiveBayes <- subset(tuning_naiveBayes_df, success == max(success))[1,]
print(tuning_naiveBayes)
```

In this case, Laplace smoothing does not affect the one number summary metrics whatsoever. To appreciate this, a graph can be plotted - for illustration purposes, said plot is performed via two different libraries: <strong>highcharter</strong> and <strong>ggplot2</strong>.

#### {.tabset}

##### highcharter

``` {r naiveBayes_1_3}
sub_naiveBayes <- paste("Optimal number of laplace is", tuning_naiveBayes$laplace, "with an averaged success (balanced accuracy and F1 score) of ", tuning_naiveBayes$success)

library(highcharter)
hchart(tuning_naiveBayes_df, 'line', hcaes(laplace, success)) %>%
  hc_title(text = "Averaged success with varying Laplace values (Naive Bayes)") %>%
  hc_subtitle(text = sub_naiveBayes) %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_xAxis(title = list(text = "Number of laplace")) %>%
  hc_yAxis(title = list(text = "Averaged success"))
```

##### ggplot2

``` {r naiveBayes_1_4}
sub_naiveBayes <- paste("Optimal Laplace value is", tuning_naiveBayes$laplace, "with an averaged success (balanced accuracy and F1 score) of ", tuning_naiveBayes$success)

library(ggplot2)
ggplot(tuning_naiveBayes_df, aes(laplace, success)) + 
  geom_line() + 
  geom_point() + theme_minimal() + 
  labs(title = "Averaged success with varying Laplace values (Naive Bayes)",
       subtitle = sub_naiveBayes,
       x = "Number of trials",
       y = "Averaged success")
```

#### {-}

### 3.1.7. Conditional Inference Trees

Conditional Inference trees, also referred as unbiased recursive partitioning, is a non-parametric class of decision trees that uses a statistical theory (selection by permutation-based significance tests) in order to select variables instead of selecting the variable that maximizes an information measure (Gini coefficient or Information Gain) and thereby removes the potential bias in CART or similar decision trees.<br>

Its usage within R comes from the <strong>party</strong> (usually referred to as <strong>partykit</strong> in most documentation) and its associated <code>ctree()</code> function.

``` {r ctree_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(party)
ctree(formula,
      data,
      subset,
      weights,
      na.action = na.pass,
      offset, cluster,
      control = ctree_control(...),
      ytrafo = NULL,
      converged = NULL,
      scores = NULL,
      doFit = TRUE,
      ...
)
```

The arguments are as follows:
<ul>
  <li><strong>formula</strong>: a symbolic description of the model to be fit.</li>
  <li><strong>data</strong>: a data frame containing the variables in the model.</li>
  <li><strong>subset</strong>: an optional vector specifying a subset of observations to be used in the fitting process.</li>
  <li><strong>weights</strong>: an optional vector of weights to be used in the fitting process. Only non-negative integer valued weights are allowed.</li>
  <li><strong>offset</strong>: an optional vector of offset values.</li>
  <li><strong>cluster</strong>: an <strong>EXPERIMENTAL</strong> and optional factor indicating independent clusters.</li>
  <li><strong>na.action</strong>: a function which indicates what should happen when the data contain missing value.</li>
  <li><strong>control</strong>: a list with control parameters.</li>
  <li><strong>ytrafo</strong>: an optional named list of functions to be applied to the response variable(s) before testing their association with the explanatory variables.</li>
  <li><strong>converged</strong>: an optional function for checking user-defined criteria before splits are implemented.</li>
  <li><strong>scores</strong>: an optional named list of scores to be attached to ordered factors.</li>
  <li><strong>doFit</strong>: if <code class = "boolean">FALSE</code>, the tree is not fitted.</li>
</ul>

More information about this function, its behavior and its arguments can be found in its associated RDocumentation page: https://www.rdocumentation.org/packages/partykit/versions/1.2-15/topics/ctree <br>

The following code snippet showcases the construction of the model, its associated prediction and confusion matrix and the most relevant one number summary metrics:

``` {r ctree_1_1}
library(party)
library(caret)

model_ctree <- ctree(diagnosis~., data=train_set)
prediction_ctree <- predict(model_ctree, test_set[,-1])
cm_ctree <- confusionMatrix(prediction_ctree, test_set$diagnosis)
cm_ctree

acc_ctree <- cm_ctree$byClass['Balanced Accuracy']
F1_ctree <- cm_ctree$byClass['F1']
print(c(acc_ctree, F1_ctree))
```

The <code>control</code> parameter allows users to tweak the function through the [ctree_control](https://www.rdocumentation.org/packages/partykit/versions/1.2-15/topics/ctree_control) function and its <code>maxdepth</code> argument. Said argument determines the number of features around which to build the decision tree,  and as such it should plateau at the optimal number of significant features.  
The following code snippet loops through values to evaluate how the one number summary metrics change with the tree's depth.

``` {r ctree_1_2}
acc_ctree_array <- NULL
F1_ctree_array <- NULL

for(i in 1:50){
    model_ctree_temp <- ctree(diagnosis~., data=train_set, controls=ctree_control(maxdepth=i)) 
    prediction_ctree_temp <- predict(model_ctree_temp, test_set[,-1]) 
    cm_ctree_temp <- confusionMatrix(prediction_ctree_temp, test_set$diagnosis)
    acc_ctree_array[i] <- cm_ctree_temp$byClass['Balanced Accuracy']
    F1_ctree_array[i] <- cm_ctree_temp$byClass['F1']
}

acc_ctree_df <- data.frame(depth = seq(1,50), acc = acc_ctree_array)
acc_ctree_optimal <- subset(acc_ctree_df, acc == max(acc))[1,]

F1_ctree_df <- data.frame(depth = seq(1,50), F1 = F1_ctree_array)
F1_ctree_optimal <- subset(F1_ctree_df, F1 == max(F1))[1,]

print(c(acc_ctree_optimal, F1_ctree_optimal)) # At the time of writing these values coincide, but that might not be the case

tuning_ctree_df <- data.frame(depth = seq(1,50), success = 0.5 * (acc_ctree_array + F1_ctree_array)) # We average F1 and balanced accuracy to measure success
library(dplyr) # For the mutate() function used to add balanced accuracy and F1 values to the dataframe
tuning_ctree <- subset(tuning_ctree_df, success == max(success))[1,] %>% mutate(acc = acc_ctree_df[max(depth), 2], F1 = F1_ctree_df[max(depth), 2])
print(tuning_ctree)
```

A graph can be plotted to appreciate the depth's effect on the algorithm' success. For illustration purposes, said plot is performed via two different libraries: <strong>highcharter</strong> and <strong>ggplot2</strong>.

#### {.tabset}

##### highcharter

``` {r ctree_1_3}
sub_ctree <- paste("Optimal depth value is", tuning_ctree$depth, "with an averaged success (balanced accuracy and F1 score) of ", tuning_ctree$success)

library(highcharter)
hchart(tuning_ctree_df, 'line', hcaes(depth, success)) %>%
  hc_title(text = "Averaged success with varying depth (Conditional Inference Trees)") %>%
  hc_subtitle(text = sub_ctree) %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_xAxis(title = list(text = "Depth value")) %>%
  hc_yAxis(title = list(text = "Averaged success"))
```

##### ggplot2

``` {r ctree_1_4}
sub_ctree <- paste("Optimal depth value is", tuning_ctree$depth, "with an averaged success (balanced accuracy and F1 score) of ", tuning_ctree$success)

library(ggplot2)
ggplot(tuning_ctree_df, aes(depth, success)) + 
  geom_line() + 
  geom_point() + theme_minimal() + 
  labs(title = "Averaged success with varying depth (Conditional Inference Trees)",
       subtitle = sub_ctree,
       x = "Depth value",
       y = "Averaged success")
```

#### {-}

These results evidentiate that the most reasonable value for the <code>maxdepth</code> argument is `r tuning_ctree$depth` and any further increase in said value does not yield an improvement.<br>

There are many other possible tweaks to better tune the function's behavior. However, the current scope of this document does not cover them (might update it in the future). As of now, refer to the provided links to better understand the function and its arguments.

### 3.1.8. randomForest

Decision trees tend to suffer from high variance: if the dataset is split into two halves, applying a decision tree to both halves could yield quite different results. One method that can be used in order to reduce the variance of a single decision tree is to make use of a random forest model. With the random forest approach a large number of decision trees are created, and every observation is fed into every decision tree with the most common outcome for each observation being used as the final output. Every new observation is fed into all the trees so that predictions are built upon a majority vote (with each and every tree being participant in said resolution).<br>

The go-to library for random forest machine learning usage is <strong>randomForest</strong>, with its core function being <code>randomForest()</code>.

``` {r randomForest_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
randomForest(x, 
             y=NULL,  
             xtest=NULL, 
             ytest=NULL, 
             ntree=500,
             na.action=na.fail,
             mtry = if (!is.null(y) && !is.factor(y))
                        max(floor(ncol(x)/3), 1)
                    else
                        floor(sqrt(ncol(x))),
             replace=TRUE, 
             classwt=NULL, 
             cutoff, 
             strata,
             sampsize = if (replace) nrow(x) else ceiling(.632*nrow(x)),
             nodesize = if (!is.null(y) && !is.factor(y)) 5 else 1,
             maxnodes = NULL,
             importance=FALSE, 
             localImp=FALSE, 
             nPerm=1,
             proximity, 
             oob.prox=proximity,
             norm.votes=TRUE, 
             do.trace=FALSE,
             keep.forest=!is.null(y) && is.null(xtest), 
             corr.bias=FALSE,
             keep.inbag=FALSE, 
             ...
)
```

The arguments are as follows:
<ul>
  <li><strong>x</strong>: a data frame or matrix of predictors.</li>
  <li><strong>y</strong>: a response vector. If a factor, classification is assumed, otherwise regression is assumed. If omitted, the function will run in unsupervised mode.</li>
  <li><strong>xtest</strong>: a data frame or matrix containing predictors for the test set.</li>
  <li><strong>ytest</strong>: response for the test set.</li>
  <li><strong>ntree</strong>: number of trees to "grow" (to be used in the forest for the prediction voting process).</li>
  <li><strong>na.action</strong>: the default action deletes all observations for which <code>y</code> is missing, but keeps those in which one or more predictors are missing.</li>
  <li><strong>mtry</strong>: number of variables randomly sampled as candidates at each split.</li>
  <li><strong>replace</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to sample the cases with replacement or not.</li>
  <li><strong>classwt</strong>: priors of the classes (ignored for regression).</li>
  <li><strong>cutoff</strong>: a vector of length equal to number of classes (and thus only useful in classification exercises). The winning class for an observation is the one with the maximum ratio of proportion of votes to cutoff. Default is <code>1/k</code> where <code>k</code> is the number of classes (i.e., majority vote wins).</li>
  <li><strong>strata</strong>: a (factor) variable that is used for stratified sampling.</li>
  <li><strong>sampsize</strong>: size(s) of sample to draw.</li>
  <li><strong>nodesize</strong>: minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5).</li>
  <li><strong>maxnodes</strong>: maximum number of terminal nodes trees in the forest can have. If not given, trees are grown to the maximum possible (limited by nodes' size).</li>
  <li><strong>importance</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to assess the importance of predictors.</li>
  <li><strong>localImp</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to compute casewire importance measures.</li>
  <li><strong>nPerm</strong>: number of times the out-of-bag data are permuted per tree for assessing variable importance.</li>
  <li><strong>proximity</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to measure the proximity between rows.</li>
  <li><strong>oob.prox</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to calculate proximity using only out-of-bag data.</li>
  <li><strong>norm.votes</strong>: if <code class = "boolean">TRUE</code> (default), the final result of votes are expressed as fractions. If <code class = "boolean">FALSE</code>, raw vote counts are returned (useful for combining results from different runs). Ignored for regression exercises.</li>
  <li><strong>do.trace</strong>: if <code class = "boolean">TRUE</code>, a more verbose output output takes place.</li>
  <li><strong>keep.forest</strong>: if <code class = "boolean">FALSE</code>, the forest will not be retained in the output object.</li>
  <li><strong>corr.bias</strong>: an <strong>EXPERIMENTAL</strong> argument to perform bias correction in regression exercises.</li>
  <li><strong>keep.inbag</strong>: <code class = "boolean">TRUE</code> or <code class = "boolean">FALSE</code> determines whether to return a matrix with dimensions <code>n</code>x<code>ntree</code> that keeps track of which samples are "in-bag" in which trees.</li>
</ul>

As can be observed, random forests (and <code>randomForest()</code> particularly) have an overwhelming amount of customazibility, mainly due to their potential complexity. If needed/interested, refer to the associated RDocumentation page for more information about this function, its behavior and its arguments: https://www.rdocumentation.org/packages/e1071/versions/1.7-9/topics/naiveBayes <br>

The following code snippet showcases the construction of the model, its associated prediction and confusion matrix and the most relevant one number summary metrics (arguments set at default):

``` {r randomForest_1_1}
library(randomForest)
library(caret)

model_randomForest <- randomForest(x = train_set[,-1], y = train_set$diagnosis)
prediction_randomForest <- predict(model_randomForest, test_set[,-1], type = "class")
cm_randomForest <- confusionMatrix(prediction_randomForest, test_set$diagnosis)
cm_randomForest

acc_randomForest <- cm_randomForest$byClass['Balanced Accuracy']
F1_randomForest <- cm_randomForest$byClass['F1']
print(c(acc_randomForest, F1_randomForest))
```

The aforementioned complexity of random forests can make the process of tuning the algorithm's behavior overwhelming and detailing every intricacy goes beyond the scope of this document. However, it is worth noting that there is a distinct correlation between the number of trees and the predictions' prevalence, up until a given number up from which there is no significant accuracy gains - there's only an increasing computing time requirement. That number can be as low as one (tree), and selecting the lowest value possible with an acceptable prevalence can save precious processing time.  
The following code snippet aims to find said number of trees:

``` {r randomForest_1_2}
library(randomForest)
library(caret)

acc_randomForest_array <- NULL
F1_randomForest_array <- NULL
prevalence_randomForest_array <- NULL

for(i in 0:50){
    model_randomForest_temp <- naiveBayes(train_set[,-1], train_set$diagnosis, ntree = i*15 + 1, importance = TRUE, proximity = TRUE) 
    prediction_randomForest_temp <- predict(model_randomForest_temp, test_set[,-1]) 
    cm_randomForest_temp <- confusionMatrix(prediction_randomForest_temp, test_set$diagnosis)
    acc_randomForest_array[i+1] <- cm_randomForest_temp$byClass['Balanced Accuracy']
    F1_randomForest_array[i+1] <- cm_randomForest_temp$byClass['F1']
    prevalence_randomForest_array[i+1] <- cm_randomForest_temp$byClass['Prevalence']
}

acc_randomForest_df <- data.frame(ntree = seq(1, length.out = 51, by = 15), acc = acc_randomForest_array)
acc_randomForest_optimal <- subset(acc_randomForest_df, acc == max(acc))[1,]

F1_randomForest_df <- data.frame(ntree = seq(1, length.out = 51, by = 15), F1 = F1_randomForest_array)
F1_randomForest_optimal <- subset(F1_randomForest_df, F1 == max(F1))[1,]

prevalence_randomForest_df <- data.frame(ntree = seq(1, length.out = 51, by = 15), prevalence = prevalence_randomForest_array)
prevalence_randomForest_optimal <- subset(prevalence_randomForest_df, prevalence == max(prevalence))[1,]

print(c(acc_randomForest_optimal, F1_randomForest_optimal, prevalence_randomForest_optimal))

# We average F1, balanced accuracy and prevalence to measure success
tuning_randomForest_df <- data.frame(ntree = seq(1, by = 15), success = 0.33 * (acc_randomForest_array + F1_randomForest_array + prevalence_randomForest_array))
library(dplyr) # For the mutate() function used to add balanced accuracy and F1 values to the dataframe
tuning_randomForest <- subset(tuning_randomForest_df, success == max(success))[1,] %>% 
  mutate(acc = acc_randomForest_df[max(ntree), 2], F1 = F1_randomForest_df[max(ntree), 2], prevalence = prevalence_randomForest_df[max(ntree), 2])
print(tuning_randomForest)
```

<!-- #### {.tabset}

##### highcharter

``` {r randomForest_1_3}
sub_naiveBayes <- paste("Optimal number of laplace is", tuning_naiveBayes$laplace, "with an averaged success (balanced accuracy and F1 score) of ", tuning_naiveBayes$success)

library(highcharter)
hchart(tuning_naiveBayes_df, 'line', hcaes(laplace, success)) %>%
  hc_title(text = "Averaged success with varying trials (Naive Bayes)") %>%
  hc_subtitle(text = sub_naiveBayes) %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_xAxis(title = list(text = "Number of laplace")) %>%
  hc_yAxis(title = list(text = "Averaged success"))
```

##### ggplot2

``` {r randomForest_1_4}
sub_naiveBayes <- paste("Optimal number of laplace is", tuning_naiveBayes$laplace, "with an averaged success (balanced accuracy and F1 score) of ", tuning_naiveBayes$success)

library(ggplot2)
ggplot(tuning_naiveBayes_df, aes(laplace, success)) + 
  geom_line() + 
  geom_point() + theme_minimal() + 
  labs(title = "Averaged success with varying trials (Naive Bayes)",
       subtitle = sub_naiveBayes,
       x = "Number of trials",
       y = "Averaged success")
```

#### {-} -->

### 3.1.9. K-Nearest Neighbors

K-Nearest Neighbors (KNN) is built around Euclidian distances. Using KNN, for any point <code>(x1, x2)</code> for which an estimate <code>p(x1, x2)</code> is wanted, the algorithm looks for the K nearest points to <code>(x1, x2)</code> and computes an average of the 0s and 1s associated with these points. This set of points used to compute the average as the neighborhood. Larger values of K result in smoother estimates, while smaller values of K result in more flexible and wiggly estimates.<br>

As with any of the other machine learning approaches covered by this document, KNN can be used within R via different libraries. The ones to be used are the following:
<ul>
  <li>The <code>caret</code> package, which includes the <code>knn3()</code> function.</li>
  <li>The <code>class</code> package, which includes the <code>knn()</code> function.</li>
</ul>

Let's detail the former:

``` {r knn3_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(caret)

# For formula
knn3(formula,
     data,
     k = 5,
     subset,
     na.action,
     ...
)

# For dataframes and matrices
knn3(x,
     y,
     k = 5,
     ...
)
```

Let's detail its arguments:
<ul>
  <li><strong>x</strong>: a data frame or matrix of predictors.</li>
  <li><strong>y</strong>: a factor vector of training set classes.</li>
  <li><strong>formula</strong>: a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code> is the response variable and <code>rhs</code> a set of predictors.</li>
  <li><strong>data</strong>: optional data frame containing the variables in the model formula.</li>
  <li><strong>k</strong>: number of neighbors considered.</li>
  <li><strong>subset</strong>: optional vector specifying a subset of observations to be used.</li>
  <li><strong>na.action</strong>: a function which indicates what should happen when the data contain <code class = "boolean">NA</code> data.</li>
</ul>

More information about this function, its behavior and its arguments can be found in its associated RDocumentation page: https://www.rdocumentation.org/packages/caret/versions/6.0-90/topics/knn3 <br>

On the other hand, the function <code>knn()</code> from the <strong>class</strong> library goes as follows:

``` {r knn_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(class)
knn(train,
    test,
    cl,
    k = 1,
    l = 0,
    prob = FALSE,
    use.all = TRUE)
```

Let's detail its arguments:
<ul>
  <li><strong>train</strong>: matrix or data frame of training set cases.</li>
  <li><strong>test</strong>: matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case.</li>
  <li><strong>cl</strong>: factor of true classifications of training set.</li>
  <li><strong>k</strong>: number of neighbors considered.</li>
  <li><strong>l</strong>: minimum vote for definite decision (meaning that less than <code>k</code> dissenting votes are allowed).</li>
  <li><strong>prob</strong>: if <code class = "boolean">TRUE</code>, the proportion of the votes for the winning class are returned as attribute <code>prob</code>.</li>
</ul>

More information about this function, its behavior and its arguments can be found in its associated RDocumentation page: https://www.rdocumentation.org/packages/class/versions/7.3-20/topics/knn <br>

The following code snippet showcases both functions, where the number of neighbors has been set at 5 in both cases in order to properly compare their results. Note that <code>knn()</code> returns the prediction directly whereas previous functions build a model which is then used to construct a prediction array.

``` {r knn_1_1}
library(caret)

model_knn3 <- knn3(x = train_set[,-1], y = train_set$diagnosis, k = 5)
prediction_knn3 <- predict(model_knn3, test_set[,-1], type = "class")
cm_knn3 <- confusionMatrix(prediction_knn3, test_set$diagnosis)
cm_knn3

acc_knn3 <- cm_knn3$byClass['Balanced Accuracy']
F1_knn3 <- cm_knn3$byClass['F1']
print(c(acc_knn3, F1_knn3))

library(class)

prediction_knn <- knn(train = train_set[,-1], test = test_set[,-1], cl = train_set$diagnosis, k = 5)
cm_knn <- confusionMatrix(prediction_knn, test_set$diagnosis)
cm_knn

acc_knn <- cm_knn$byClass['Balanced Accuracy']
F1_knn <- cm_knn$byClass['F1']
print(c(acc_knn, F1_knn))
```

Note that results are identical with both functions, as should be (core-wise they should be identical).<br>

KNN can be fine-tuned by selecting an optimal number of neighbors. Said tuning process is showcased in the following code snippet (only <code>knn3()</code> is used now):

``` {r knn_1_2}
library(caret)
acc_knn3_array <- NULL
F1_knn3_array <- NULL

for(i in 1:50){
    model_knn3_temp <- knn3(x = train_set[,-1], y = train_set$diagnosis, k = i)     
    prediction_knn3_temp <- predict(model_knn3_temp, test_set[,-1], type = "class") 
    cm_knn3_temp <- confusionMatrix(prediction_knn3_temp, test_set$diagnosis)
    acc_knn3_array[i] <- cm_knn3_temp$byClass['Balanced Accuracy']
    F1_knn3_array[i] <- cm_knn3_temp$byClass['F1']
}

acc_knn3_df <- data.frame(k = seq(1,50), acc = acc_knn3_array)
acc_knn3_optimal <- subset(acc_knn3_df, acc == max(acc))[1,]

F1_knn3_df <- data.frame(k = seq(1,50), F1 = F1_knn3_array)
F1_knn3_optimal <- subset(F1_knn3_df, F1 == max(F1))[1,]

print(c(acc_knn3_optimal, F1_knn3_optimal)) # At the time of writing these values coincide, but that might not be the case

tuning_knn3_df <- data.frame(k = seq(1,50), success = 0.5 * (acc_knn3_array + F1_knn3_array)) # We average F1 and balanced accuracy to measure success
library(dplyr) # For the mutate() function used to add balanced accuracy and F1 values to the dataframe
tuning_knn3 <- subset(tuning_knn3_df, success == max(success))[1,] %>% mutate(acc = acc_knn3_df[max(k), 2], F1 = F1_knn3_df[max(k), 2])
print(tuning_knn3)
```

The best results are obtained with <code>k = `r tuning_knn3$k`</code>, which might seem lower than expected but it does make sense since, as was already stated, smaller values of K result in more flexible and wiggly estimates.  
These values and their associated results can be graphically interpreted with a plot. For illustration purposes, said plot is performed via two different libraries: <strong>highcharter</strong> and <strong>ggplot2</strong>.

#### {.tabset}

##### highcharter

``` {r knn_1_3}
sub_knn3 <- paste("Optimal number of neighbors is", tuning_knn3$k, "with an averaged success (balanced accuracy and F1 score) of ", tuning_knn3$success)

library(highcharter)
hchart(tuning_knn3_df, 'line', hcaes(k, success)) %>%
  hc_title(text = "Averaged success with varying k (KNN)") %>%
  hc_subtitle(text = sub_knn3) %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_xAxis(title = list(text = "K neighbors")) %>%
  hc_yAxis(title = list(text = "Averaged success"))
```

##### ggplot2

``` {r knn_1_4}
sub_knn3 <- paste("Optimal number of neighbors is", tuning_knn3$k, "with an averaged success (balanced accuracy and F1 score) of ", tuning_knn3$success)

library(ggplot2)
ggplot(tuning_knn3_df, aes(k, success)) + 
  geom_line() + 
  geom_point() + theme_minimal() + 
  labs(title = "Averaged success with varying k (KNN)",
       subtitle = sub_knn3,
       x = "K neighbors",
       y = "Averaged success")
```

#### {-}

More machine learning approaches/algorithms to be included in a later update.

<!-- ### 3.1.10. K-Means

``` {r ML_kmeans_1, eval = FALSE}
predict.kmeans <- function(newdata, object){
    centers <- object$centers
    n_centers <- nrow(centers)
    dist_mat <- as.matrix(dist(rbind(centers, newdata)))
    dist_mat <- dist_mat[-seq(n_centers), seq(n_centers)]
    max.col(-dist_mat)
}
```

### 3.1.11. GBM

``` {r ML_GBM_1, eval = FALSE}
library(gbm)
test_gbm <- gbm(diagnosis~., data=train, distribution="gaussian",n.trees = 10000,
                shrinkage = 0.01, interaction.depth = 4, bag.fraction=0.5, train.fraction=0.5,n.minobsinnode=10,cv.folds=3,keep.data=TRUE,verbose=FALSE,n.cores=1)
best.iter <- gbm.perf(test_gbm, method="cv",plot.it=FALSE)
fitControl = trainControl(method="cv", number=5, returnResamp="all")
learn_gbm = train(diagnosis~., data=train, method="gbm", distribution="bernoulli", trControl=fitControl, verbose=F, tuneGrid=data.frame(.n.trees=best.iter, .shrinkage=0.01, .interaction.depth=1, .n.minobsinnode=1))
pre_gbm <- predict(learn_gbm, test[,-1])
cm_gbm <- confusionMatrix(pre_gbm, test$diagnosis)
cm_gbm
```

### 3.1.12. adaBoost

``` {r ML_adaBoost_1, eval = FALSE}
library(rpart)
library(ada)
control <- rpart.control(cp = -1, maxdepth = 14,maxcompete = 1,xval = 0)
learn_ada <- ada(diagnosis~., data = train, test.x = train[,-1], test.y = train[,1], type = "gentle", control = control, iter = 70)
pre_ada <- predict(learn_ada, test[,-1])
cm_ada <- confusionMatrix(pre_ada, test$diagnosis)
cm_ada
```

### 3.1.13. SVM

``` {r ML_SVM_1, eval = FALSE}
learn_svm <- svm(diagnosis~., data=train)
pre_svm <- predict(learn_svm, test[,-1])
cm_svm <- confusionMatrix(pre_svm, test$diagnosis)
cm_svm
```


``` {r ML_SVM_Tune_1, eval = FALSE}
gamma <- seq(0,0.1,0.005)
cost <- 2^(0:5)
parms <- expand.grid(cost=cost, gamma=gamma)    ## 231

acc_test <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL

for(i in 1:NROW(parms)){        
        learn_svm <- svm(diagnosis~., data=train, gamma=parms$gamma[i], cost=parms$cost[i])
        pre_svm <- predict(learn_svm, test[,-1])
        accuracy1 <- confusionMatrix(pre_svm, test$diagnosis)
        accuracy2[i] <- accuracy1$overall[1]
}

acc <- data.frame(p= seq(1,NROW(parms)), cnt = accuracy2)

opt_p <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of parameter is", opt_p$p, "(accuracy :", opt_p$cnt,") in SVM")

library(highcharter)
hchart(acc, 'line', hcaes(p, cnt)) %>%
  hc_title(text = "Accuracy With Varying Parameters (SVM)") %>%
  hc_subtitle(text = sub) %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_xAxis(title = list(text = "Number of Parameters")) %>%
  hc_yAxis(title = list(text = "Accuracy"))
```

### 3.1.13. PST and tree prune.

The <strong>PST</strong> is a lesser known package which stands for Probabilistic Suffix Trees and is built around the <strong>pruning</strong> aspect of decision trees. As the name implies, pruning involves cutting back the tree, which is useful if said tree is <strong>overfitted</strong>: the decision tree algorithm repeatedly partitions data into smaller and smaller subsets until those final subsets are homogeneous in terms of the outcome variable, which in practice often means that the final subsets (the tree leaves) each consist of only one or a few data points - although the tree has learned the data exactly, a new data point that differs very slightly might not be predicted well since it does not fit within any of the leaves' very restrictive and precise constrains (hence why the tree is "overfit").<br>

As already stated, pruning avoids overfitting by cutting back the tree which in turn could help the model achieve a higher accuracy not only within the confusion matrix but, most importantly, with regards to new data entries.<br>

``` {r prune_0, eval = FALSE}
# Do not run this code snippet, as it is only here for illustration purposes
library(PST)
prune(object, 
      nmin,
      L,
      gain,
      C,
      keep,
      drop,
      state,
      delete = TRUE,
      lik =TRUE)
```

The functions arguments are as follows:
<ul>
  <li><strong>object</strong>: the model formula (should look like <code>feature ~ predictor</code>).</li>
  <li><strong>nmin</strong>: an optional data frame in which to interpret the variables named in the formula.</li>
  <li><strong>L</strong>: optional case weights.</li>
  <li><strong>gain</strong>: optional expression saying that only a subset of the rows of the data should be used in the fit.</li>
  <li><strong>C</strong>: the default action deletes all observations for which <code>y</code> is missing, but keeps those in which one or more predictors are missing.</li>
  <li><strong>keep</strong>: the model formula (should look like <code>feature ~ predictor</code>).</li>
  <li><strong>drop</strong>: an optional data frame in which to interpret the variables named in the formula.</li>
  <li><strong>state</strong>: optional case weights.</li>
  <li><strong>delete</strong>: optional expression saying that only a subset of the rows of the data should be used in the fit.</li>
  <li><strong>lik</strong>: optional expression saying that only a subset of the rows of the data should be used in the fit.</li>
</ul>

``` {r prune_1, eval = FALSE}
model_rpart_2 <- prune(object = model_rpart, cp = model_rpart$cptable[which.min(model_rpart$cptable[,"xerror"]),"CP"])
pre_pru <- predict(learn_pru, test[,-1], type="class")
cm_pru <-confusionMatrix(pre_pru, test$diagnosis)           
cm_pru
```

-->

## 3.2. Results

The following code snippet plots the overall success of each approach. It does so with a "Four fold" plot where each confusion matrix can be visually evaluated: the four folds represent each section of the confusion matrix table, with the blue sections being correct guesses (on one side, benign being predicted as benign; on the other, malign being predicted as malign) and the red sections correspond to incorrect guesses (benign being predicted as malign and vice-versa).  
The overall success of each approach is represented by the average of their balanced accuracy and their F1 score (as a percentage).

```{r ml_visual_comparison}
# Visualize to compare the accuracy of all methods
col <- c("#ed3b3b", "#0099ff")
par(mfrow=c(4,4))
fourfoldplot(cm_linear$table, color = col, conf.level = 0, margin = 1, main=paste("Linear Model (",round(mean(acc_linear, F1_linear)*100, 2),"%)",sep=""))
fourfoldplot(cm_logistic$table, color = col, conf.level = 0, margin = 1, main=paste("Logistic Model (",round(mean(acc_logistic, F1_logistic)*100, 2),"%)",sep=""))
fourfoldplot(cm_C50$table, color = col, conf.level = 0, margin = 1, main=paste("C5.0 (",round(mean(acc_C50, F1_C50)*100, 2),"%)",sep=""))
fourfoldplot(cm_rpart$table, color = col, conf.level = 0, margin = 1, main=paste("rpart (",round(mean(acc_rpart, F1_rpart)*100, 2),"%)",sep=""))

fourfoldplot(cm_JRip$table, color = col, conf.level = 0, margin = 1, main=paste("RWeka JRip (",round(mean(cm_JRip$byClass['Balanced Accuracy'], cm_JRip$byClass['F1'])*100, 2),"%)",sep=""))
fourfoldplot(cm_OneR$table, color = col, conf.level = 0, margin = 1, main=paste("RWeka OneR (",round(mean(cm_OneR$byClass['Balanced Accuracy'], cm_OneR$byClass['F1'])*100, 2),"%)",sep=""))
fourfoldplot(cm_PART$table, color = col, conf.level = 0, margin = 1, main=paste("RWeka PART (",round(mean(cm_PART$byClass['Balanced Accuracy'], cm_PART$byClass['F1'])*100, 2),"%)",sep=""))
fourfoldplot(cm_naiveBayes$table, color = col, conf.level = 0, margin = 1, main=paste("Naive Bayes (",round(mean(acc_naiveBayes, F1_naiveBayes)*100, 2),"%)",sep=""))

fourfoldplot(cm_ctree$table, color = col, conf.level = 0, margin = 1, main=paste("ctree (",round(mean(acc_ctree, F1_ctree)*100, 2),"%)",sep=""))
fourfoldplot(cm_randomForest$table, color = col, conf.level = 0, margin = 1, main=paste("Random Forest (",round(mean(acc_randomForest, F1_randomForest)*100, 2),"%)",sep=""))
fourfoldplot(cm_knn3$table, color = col, conf.level = 0, margin = 1, main=paste("KNN3 (",round(mean(acc_knn3, F1_knn3)*100, 2),"%)",sep=""))
fourfoldplot(cm_knn$table, color = col, conf.level = 0, margin = 1, main=paste("KNN (",round(mean(acc_knn, F1_knn)*100, 2),"%)",sep=""))
```

The following code snippet goes through the evaluated approaches and picks the one with the higher success.

```{r ml_best_selection}
# Select a best prediction model according to high accuracy
opt_predict <- c(mean(acc_linear, F1_linear)*100, 
mean(acc_logistic, F1_logistic)*100,
mean(acc_C50, F1_C50)*100,
mean(acc_rpart, F1_rpart)*100,
mean(cm_JRip$byClass['Balanced Accuracy'], cm_JRip$byClass['F1'])*100,
mean(cm_OneR$byClass['Balanced Accuracy'], cm_OneR$byClass['F1'])*100,
mean(cm_PART$byClass['Balanced Accuracy'], cm_PART$byClass['F1'])*100,
mean(acc_naiveBayes, F1_naiveBayes)*100,
mean(acc_ctree, F1_ctree)*100,
mean(acc_randomForest, F1_randomForest)*100,
mean(acc_knn3, F1_knn3)*100,
mean(acc_knn, F1_knn)*100)

names(opt_predict) <- c("Linear Model",
"Logistic Model",
"C5.0",
"rpart",
"RWeka JRip",
"RWeka OneR",
"RWeka PART",
"Naive Bayes",
"ctree",
"Random Forest",
"KNN3",
"KNN")

best_predict_model <- subset(opt_predict, opt_predict==max(opt_predict))
best_predict_model
```